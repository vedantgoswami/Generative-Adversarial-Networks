# Generative-Adversarial-Networks
Generative Adversarial Networks for Generating MNIST digits.

Before explaining the Project, I would like to give a brief introduction about GAN's

## What is GAN's ?
Generative Adversarial Networks, or GANs are a model architecture for training a generative model, and it is most common to use deep learning models in this architecture.
<b>Yann LeCun</b> described it as <b>‚Äúthe most interesting idea in the last 10 years in Machine Learning‚Äù</b><br>
<p>
In short GANs can create images that look like photographs of human faces, even though the faces don‚Äôt belong to any real person just like the <b>Monalisa</b>.
</p>
<p align="center"><b> Here are some fake images generated by High End GAN's <b></p>
<img src="https://github.com/vedantgoswami/Generative-Adversarial-Networks/blob/main/Images/fake_ai_faces.0.png">
  
## <u>GAN Architecture</u>
<img src="https://github.com/vedantgoswami/Generative-Adversarial-Networks/blob/main/Images/1_6zMZBE6xtgGUVqkaLTBaJQ.png">
  
GAN basically contains two networks a <b>Generator</b>, a <b>Discriminator</b> competining against each other.<br>
* The generator network is trained to fool the discriminator, it wants to output data that looks as close as possible to real, training data.
* The discriminator is a classifier that is trained to figure out which data is real and which is fake.
<p>
The general structure of a GAN is shown in the diagram above, using MNIST images as data. The latent sample is a random vector that the generator uses to construct its fake images.This latent vector come from the normal distribution.
This is often called a <b>latent vector</b> and that vector space is called <b>latent space</b>. As the generator trains, it figures out how to map latent vectors to recognizable images that can fool the discriminator.</p>

# Explaining my GAN network for MNIST digits Generation.
Neccessary import for this model:
```
* Tensoflow
* keras
* Numpy
```
<b> For this network i would recommend to GPU for faster training </b>

Defining the input shape
```
img_rows = 28
img_cols = 28
channels = 1    # Gray Scale Images
img_shape = (img_rows,img_cols,channels)
```
## Generator Model
The input of the generator will be noise of shape (100,1) from the Normal distribution and it will output an image of same size as that of training data.
```
model.add(Dense(256,input_shape=noise_shape))
model.add(LeakyReLU(alpha=0.2))
model.add(BatchNormalization(momentum=0.8))
```
The generator model had the Dense layer with Leaky ReLU as the activation function to avoid the problem of <b>dead ReLU or dying ReLU</b>
The output of generator model is 28*28 = 786 pixels 
I had used tanh activation function to our output layer because generator has been found to perform the best with ùë°ùëéùëõ‚Ñé for the generator output, which scales the output to be between -1 and 1, instead of 0 and 1.
## Discriminator Model
The input of the discriminator is an image and the output is the validity, the likelihood of the image being real.OR Simply <b> Binary Classification</b>.
```
model= Sequential()
model.add(Flatten(input_shape=img_shape))
model.add(Dense(512))
model.add(LeakyReLU(alpha=0.2))
model.add(Dense(256))
model.add(LeakyReLU(alpha=0.2))
model.add(Dense(1,activation='sigmoid'))
model.summary()
```
