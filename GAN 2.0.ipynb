{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input,Dense,Reshape,Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows,img_cols,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    noise_shape = (100,)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256,input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(Dense(np.prod(img_shape),activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "    return Model(noise,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_descriminator():\n",
    "    model= Sequential()\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    img=Input(shape=img_shape)\n",
    "    validity=model(img)\n",
    "    return Model(img,validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train(epochs,batch_size=128,save_interval=50):\n",
    "        (x_train,_),(_,_)=mnist.load_data()\n",
    "        x_train = (x_train.astype(np.float32)-127.5)/127.5\n",
    "        x_train = np.expand_dims(x_train,axis=3)\n",
    "        half_batch = int(batch_size/2)\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.randint(0,x_train.shape[0],half_batch)\n",
    "            imgs = x_train[idx]\n",
    "            noise = np.random.normal(0,1,(half_batch,100)) \n",
    "            gen_imgs = generator.predict(noise)\n",
    "            d_loss_real = discriminator.train_on_batch(imgs,np.ones((half_batch,1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs,np.zeros((half_batch,1)))\n",
    "            \n",
    "            d_loss = 0.5 * np.add(d_loss_real,d_loss_fake)\n",
    "            \n",
    "            noise = np.random.normal(0,1,(batch_size,100))\n",
    "            valid_y = np.array([1]*batch_size)\n",
    "            g_loss = combined.train_on_batch(noise,valid_y)\n",
    "            \n",
    "            print(\"%d [D Loss: %f, Acc.: %0.2f%%] [G Loss: %f]\"% (epoch,d_loss[0],100*d_loss[1],g_loss))\n",
    "            \n",
    "            if(epoch % save_interval == 0 ):\n",
    "                save_imgs(epoch)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    r,c = 5,5\n",
    "    noise = np.random.normal(0,1,(r*c,100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    fig,axs = plt.subplots(r,c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt,:,:,0],cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt+=1\n",
    "    fig.savefig(\"D:\\Dataset\\Test\\mnist_%d.png\"%epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(generator, discriminator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tdiscriminator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(discriminator)\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.0002,0.5)\n",
    "discriminator = build_descriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=Input(shape=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = generator(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = define_gan(generator,discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = discriminator(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = Model(z,valid)\n",
    "combined.compile(loss='binary_crossentropy',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D Loss: 0.396719, Acc.: 71.88%] [G Loss: 0.669676]\n",
      "1 [D Loss: 0.365033, Acc.: 68.75%] [G Loss: 0.732362]\n",
      "2 [D Loss: 0.341151, Acc.: 81.25%] [G Loss: 0.787190]\n",
      "3 [D Loss: 0.338431, Acc.: 84.38%] [G Loss: 0.836685]\n",
      "4 [D Loss: 0.306261, Acc.: 81.25%] [G Loss: 0.969622]\n",
      "5 [D Loss: 0.271458, Acc.: 96.88%] [G Loss: 1.121125]\n",
      "6 [D Loss: 0.228697, Acc.: 100.00%] [G Loss: 1.232662]\n",
      "7 [D Loss: 0.218446, Acc.: 100.00%] [G Loss: 1.324153]\n",
      "8 [D Loss: 0.159859, Acc.: 100.00%] [G Loss: 1.573921]\n",
      "9 [D Loss: 0.152613, Acc.: 100.00%] [G Loss: 1.631724]\n",
      "10 [D Loss: 0.118188, Acc.: 100.00%] [G Loss: 1.756233]\n",
      "11 [D Loss: 0.109221, Acc.: 100.00%] [G Loss: 1.922555]\n",
      "12 [D Loss: 0.097611, Acc.: 100.00%] [G Loss: 2.006845]\n",
      "13 [D Loss: 0.107604, Acc.: 100.00%] [G Loss: 2.130295]\n",
      "14 [D Loss: 0.075587, Acc.: 100.00%] [G Loss: 2.250367]\n",
      "15 [D Loss: 0.075966, Acc.: 100.00%] [G Loss: 2.330700]\n",
      "16 [D Loss: 0.056928, Acc.: 100.00%] [G Loss: 2.404482]\n",
      "17 [D Loss: 0.054977, Acc.: 100.00%] [G Loss: 2.444279]\n",
      "18 [D Loss: 0.046184, Acc.: 100.00%] [G Loss: 2.594964]\n",
      "19 [D Loss: 0.052614, Acc.: 100.00%] [G Loss: 2.528370]\n",
      "20 [D Loss: 0.046215, Acc.: 100.00%] [G Loss: 2.480701]\n",
      "21 [D Loss: 0.039434, Acc.: 100.00%] [G Loss: 2.680602]\n",
      "22 [D Loss: 0.047977, Acc.: 100.00%] [G Loss: 2.808072]\n",
      "23 [D Loss: 0.046826, Acc.: 100.00%] [G Loss: 2.827028]\n",
      "24 [D Loss: 0.033003, Acc.: 100.00%] [G Loss: 2.855931]\n",
      "25 [D Loss: 0.043514, Acc.: 100.00%] [G Loss: 2.926064]\n",
      "26 [D Loss: 0.029427, Acc.: 100.00%] [G Loss: 3.009022]\n",
      "27 [D Loss: 0.042770, Acc.: 100.00%] [G Loss: 2.949241]\n",
      "28 [D Loss: 0.023398, Acc.: 100.00%] [G Loss: 3.188370]\n",
      "29 [D Loss: 0.030431, Acc.: 100.00%] [G Loss: 2.939577]\n",
      "30 [D Loss: 0.030232, Acc.: 100.00%] [G Loss: 3.120950]\n",
      "31 [D Loss: 0.025976, Acc.: 100.00%] [G Loss: 3.222129]\n",
      "32 [D Loss: 0.027018, Acc.: 100.00%] [G Loss: 3.082330]\n",
      "33 [D Loss: 0.027890, Acc.: 100.00%] [G Loss: 3.321394]\n",
      "34 [D Loss: 0.036956, Acc.: 100.00%] [G Loss: 3.153178]\n",
      "35 [D Loss: 0.033075, Acc.: 100.00%] [G Loss: 3.390191]\n",
      "36 [D Loss: 0.024297, Acc.: 100.00%] [G Loss: 3.377035]\n",
      "37 [D Loss: 0.021650, Acc.: 100.00%] [G Loss: 3.437770]\n",
      "38 [D Loss: 0.023551, Acc.: 100.00%] [G Loss: 3.428814]\n",
      "39 [D Loss: 0.021006, Acc.: 100.00%] [G Loss: 3.509567]\n",
      "40 [D Loss: 0.013424, Acc.: 100.00%] [G Loss: 3.503233]\n",
      "41 [D Loss: 0.021111, Acc.: 100.00%] [G Loss: 3.467317]\n",
      "42 [D Loss: 0.015887, Acc.: 100.00%] [G Loss: 3.472780]\n",
      "43 [D Loss: 0.019457, Acc.: 100.00%] [G Loss: 3.464563]\n",
      "44 [D Loss: 0.017761, Acc.: 100.00%] [G Loss: 3.595240]\n",
      "45 [D Loss: 0.017774, Acc.: 100.00%] [G Loss: 3.709508]\n",
      "46 [D Loss: 0.013481, Acc.: 100.00%] [G Loss: 3.577105]\n",
      "47 [D Loss: 0.012030, Acc.: 100.00%] [G Loss: 3.609177]\n",
      "48 [D Loss: 0.024685, Acc.: 100.00%] [G Loss: 3.702090]\n",
      "49 [D Loss: 0.015941, Acc.: 100.00%] [G Loss: 3.658412]\n",
      "50 [D Loss: 0.013082, Acc.: 100.00%] [G Loss: 3.740491]\n",
      "51 [D Loss: 0.015823, Acc.: 100.00%] [G Loss: 3.660738]\n",
      "52 [D Loss: 0.016709, Acc.: 100.00%] [G Loss: 3.688281]\n",
      "53 [D Loss: 0.020658, Acc.: 100.00%] [G Loss: 3.799302]\n",
      "54 [D Loss: 0.013463, Acc.: 100.00%] [G Loss: 3.935810]\n",
      "55 [D Loss: 0.012722, Acc.: 100.00%] [G Loss: 3.987736]\n",
      "56 [D Loss: 0.017451, Acc.: 100.00%] [G Loss: 3.877147]\n",
      "57 [D Loss: 0.017953, Acc.: 100.00%] [G Loss: 3.853486]\n",
      "58 [D Loss: 0.016188, Acc.: 100.00%] [G Loss: 3.902310]\n",
      "59 [D Loss: 0.014667, Acc.: 100.00%] [G Loss: 3.958278]\n",
      "60 [D Loss: 0.012440, Acc.: 100.00%] [G Loss: 3.999252]\n",
      "61 [D Loss: 0.012440, Acc.: 100.00%] [G Loss: 4.010174]\n",
      "62 [D Loss: 0.020186, Acc.: 100.00%] [G Loss: 3.982595]\n",
      "63 [D Loss: 0.014071, Acc.: 100.00%] [G Loss: 4.096912]\n",
      "64 [D Loss: 0.011150, Acc.: 100.00%] [G Loss: 4.186500]\n",
      "65 [D Loss: 0.010128, Acc.: 100.00%] [G Loss: 4.202201]\n",
      "66 [D Loss: 0.010200, Acc.: 100.00%] [G Loss: 4.098370]\n",
      "67 [D Loss: 0.012437, Acc.: 100.00%] [G Loss: 4.049714]\n",
      "68 [D Loss: 0.015342, Acc.: 100.00%] [G Loss: 4.247672]\n",
      "69 [D Loss: 0.010969, Acc.: 100.00%] [G Loss: 4.144409]\n",
      "70 [D Loss: 0.010009, Acc.: 100.00%] [G Loss: 4.270989]\n",
      "71 [D Loss: 0.009473, Acc.: 100.00%] [G Loss: 4.268665]\n",
      "72 [D Loss: 0.017215, Acc.: 100.00%] [G Loss: 4.355706]\n",
      "73 [D Loss: 0.010360, Acc.: 100.00%] [G Loss: 4.313736]\n",
      "74 [D Loss: 0.008620, Acc.: 100.00%] [G Loss: 4.311818]\n",
      "75 [D Loss: 0.012511, Acc.: 100.00%] [G Loss: 4.173382]\n",
      "76 [D Loss: 0.021879, Acc.: 100.00%] [G Loss: 4.340304]\n",
      "77 [D Loss: 0.008215, Acc.: 100.00%] [G Loss: 4.253994]\n",
      "78 [D Loss: 0.009178, Acc.: 100.00%] [G Loss: 4.309683]\n",
      "79 [D Loss: 0.012581, Acc.: 100.00%] [G Loss: 4.333871]\n",
      "80 [D Loss: 0.013429, Acc.: 100.00%] [G Loss: 4.365439]\n",
      "81 [D Loss: 0.009358, Acc.: 100.00%] [G Loss: 4.388176]\n",
      "82 [D Loss: 0.007811, Acc.: 100.00%] [G Loss: 4.382760]\n",
      "83 [D Loss: 0.009130, Acc.: 100.00%] [G Loss: 4.298070]\n",
      "84 [D Loss: 0.006962, Acc.: 100.00%] [G Loss: 4.419255]\n",
      "85 [D Loss: 0.012573, Acc.: 100.00%] [G Loss: 4.387585]\n",
      "86 [D Loss: 0.008629, Acc.: 100.00%] [G Loss: 4.392046]\n",
      "87 [D Loss: 0.005278, Acc.: 100.00%] [G Loss: 4.468289]\n",
      "88 [D Loss: 0.009414, Acc.: 100.00%] [G Loss: 4.258345]\n",
      "89 [D Loss: 0.008836, Acc.: 100.00%] [G Loss: 4.483053]\n",
      "90 [D Loss: 0.016238, Acc.: 100.00%] [G Loss: 4.348765]\n",
      "91 [D Loss: 0.007162, Acc.: 100.00%] [G Loss: 4.421180]\n",
      "92 [D Loss: 0.008676, Acc.: 100.00%] [G Loss: 4.407661]\n",
      "93 [D Loss: 0.008221, Acc.: 100.00%] [G Loss: 4.309207]\n",
      "94 [D Loss: 0.009337, Acc.: 100.00%] [G Loss: 4.436048]\n",
      "95 [D Loss: 0.005448, Acc.: 100.00%] [G Loss: 4.449825]\n",
      "96 [D Loss: 0.007874, Acc.: 100.00%] [G Loss: 4.615300]\n",
      "97 [D Loss: 0.012682, Acc.: 100.00%] [G Loss: 4.538368]\n",
      "98 [D Loss: 0.010531, Acc.: 100.00%] [G Loss: 4.664150]\n",
      "99 [D Loss: 0.009310, Acc.: 100.00%] [G Loss: 4.645386]\n",
      "100 [D Loss: 0.012462, Acc.: 100.00%] [G Loss: 4.449788]\n",
      "101 [D Loss: 0.010577, Acc.: 100.00%] [G Loss: 4.562202]\n",
      "102 [D Loss: 0.007816, Acc.: 100.00%] [G Loss: 4.405906]\n",
      "103 [D Loss: 0.011437, Acc.: 100.00%] [G Loss: 4.512598]\n",
      "104 [D Loss: 0.009426, Acc.: 100.00%] [G Loss: 4.538003]\n",
      "105 [D Loss: 0.007189, Acc.: 100.00%] [G Loss: 4.480634]\n",
      "106 [D Loss: 0.009038, Acc.: 100.00%] [G Loss: 4.536274]\n",
      "107 [D Loss: 0.007072, Acc.: 100.00%] [G Loss: 4.555875]\n",
      "108 [D Loss: 0.006533, Acc.: 100.00%] [G Loss: 4.576165]\n",
      "109 [D Loss: 0.010527, Acc.: 100.00%] [G Loss: 4.312309]\n",
      "110 [D Loss: 0.012453, Acc.: 100.00%] [G Loss: 4.446568]\n",
      "111 [D Loss: 0.007553, Acc.: 100.00%] [G Loss: 4.718833]\n",
      "112 [D Loss: 0.009281, Acc.: 100.00%] [G Loss: 4.587446]\n",
      "113 [D Loss: 0.012631, Acc.: 100.00%] [G Loss: 4.690856]\n",
      "114 [D Loss: 0.007335, Acc.: 100.00%] [G Loss: 4.720138]\n",
      "115 [D Loss: 0.013493, Acc.: 100.00%] [G Loss: 4.726420]\n",
      "116 [D Loss: 0.009996, Acc.: 100.00%] [G Loss: 4.772016]\n",
      "117 [D Loss: 0.009267, Acc.: 100.00%] [G Loss: 4.570719]\n",
      "118 [D Loss: 0.014841, Acc.: 100.00%] [G Loss: 4.742644]\n",
      "119 [D Loss: 0.007523, Acc.: 100.00%] [G Loss: 4.801443]\n",
      "120 [D Loss: 0.015201, Acc.: 100.00%] [G Loss: 4.813101]\n",
      "121 [D Loss: 0.008228, Acc.: 100.00%] [G Loss: 4.716155]\n",
      "122 [D Loss: 0.010311, Acc.: 100.00%] [G Loss: 4.787624]\n",
      "123 [D Loss: 0.007922, Acc.: 100.00%] [G Loss: 4.796451]\n",
      "124 [D Loss: 0.013233, Acc.: 100.00%] [G Loss: 4.874952]\n",
      "125 [D Loss: 0.012173, Acc.: 100.00%] [G Loss: 4.839524]\n",
      "126 [D Loss: 0.010402, Acc.: 100.00%] [G Loss: 5.002020]\n",
      "127 [D Loss: 0.005198, Acc.: 100.00%] [G Loss: 4.959379]\n",
      "128 [D Loss: 0.012619, Acc.: 100.00%] [G Loss: 5.033370]\n",
      "129 [D Loss: 0.010469, Acc.: 100.00%] [G Loss: 5.097545]\n",
      "130 [D Loss: 0.008107, Acc.: 100.00%] [G Loss: 4.949811]\n",
      "131 [D Loss: 0.014986, Acc.: 100.00%] [G Loss: 4.932052]\n",
      "132 [D Loss: 0.009901, Acc.: 100.00%] [G Loss: 4.953388]\n",
      "133 [D Loss: 0.004718, Acc.: 100.00%] [G Loss: 4.900526]\n",
      "134 [D Loss: 0.010008, Acc.: 100.00%] [G Loss: 4.828404]\n",
      "135 [D Loss: 0.008219, Acc.: 100.00%] [G Loss: 4.900273]\n",
      "136 [D Loss: 0.007417, Acc.: 100.00%] [G Loss: 4.942104]\n",
      "137 [D Loss: 0.013192, Acc.: 100.00%] [G Loss: 5.080724]\n",
      "138 [D Loss: 0.016750, Acc.: 100.00%] [G Loss: 5.185604]\n",
      "139 [D Loss: 0.015567, Acc.: 100.00%] [G Loss: 5.194748]\n",
      "140 [D Loss: 0.005468, Acc.: 100.00%] [G Loss: 4.848333]\n",
      "141 [D Loss: 0.005552, Acc.: 100.00%] [G Loss: 4.908631]\n",
      "142 [D Loss: 0.007403, Acc.: 100.00%] [G Loss: 4.930876]\n",
      "143 [D Loss: 0.005742, Acc.: 100.00%] [G Loss: 4.838629]\n",
      "144 [D Loss: 0.005418, Acc.: 100.00%] [G Loss: 4.850223]\n",
      "145 [D Loss: 0.008277, Acc.: 100.00%] [G Loss: 4.816806]\n",
      "146 [D Loss: 0.016148, Acc.: 100.00%] [G Loss: 5.155101]\n",
      "147 [D Loss: 0.011551, Acc.: 100.00%] [G Loss: 5.230005]\n",
      "148 [D Loss: 0.009511, Acc.: 100.00%] [G Loss: 5.041317]\n",
      "149 [D Loss: 0.024020, Acc.: 100.00%] [G Loss: 5.230716]\n",
      "150 [D Loss: 0.008617, Acc.: 100.00%] [G Loss: 5.364249]\n",
      "151 [D Loss: 0.011001, Acc.: 100.00%] [G Loss: 5.147716]\n",
      "152 [D Loss: 0.008145, Acc.: 100.00%] [G Loss: 5.099501]\n",
      "153 [D Loss: 0.005306, Acc.: 100.00%] [G Loss: 5.045986]\n",
      "154 [D Loss: 0.012741, Acc.: 100.00%] [G Loss: 5.057683]\n",
      "155 [D Loss: 0.010094, Acc.: 100.00%] [G Loss: 5.317883]\n",
      "156 [D Loss: 0.005470, Acc.: 100.00%] [G Loss: 5.287829]\n",
      "157 [D Loss: 0.013174, Acc.: 100.00%] [G Loss: 5.156292]\n",
      "158 [D Loss: 0.006462, Acc.: 100.00%] [G Loss: 4.860724]\n",
      "159 [D Loss: 0.017847, Acc.: 100.00%] [G Loss: 5.127413]\n",
      "160 [D Loss: 0.006397, Acc.: 100.00%] [G Loss: 5.042320]\n",
      "161 [D Loss: 0.013272, Acc.: 100.00%] [G Loss: 5.001050]\n",
      "162 [D Loss: 0.016414, Acc.: 100.00%] [G Loss: 5.114622]\n",
      "163 [D Loss: 0.005761, Acc.: 100.00%] [G Loss: 5.049095]\n",
      "164 [D Loss: 0.018010, Acc.: 100.00%] [G Loss: 5.050112]\n",
      "165 [D Loss: 0.011461, Acc.: 100.00%] [G Loss: 5.065218]\n",
      "166 [D Loss: 0.041582, Acc.: 100.00%] [G Loss: 5.732981]\n",
      "167 [D Loss: 0.228209, Acc.: 90.62%] [G Loss: 4.361908]\n",
      "168 [D Loss: 0.063474, Acc.: 96.88%] [G Loss: 4.800154]\n",
      "169 [D Loss: 0.010095, Acc.: 100.00%] [G Loss: 5.017035]\n",
      "170 [D Loss: 0.072698, Acc.: 96.88%] [G Loss: 5.263122]\n",
      "171 [D Loss: 0.014105, Acc.: 100.00%] [G Loss: 5.558385]\n",
      "172 [D Loss: 0.015173, Acc.: 100.00%] [G Loss: 5.288052]\n",
      "173 [D Loss: 0.025484, Acc.: 100.00%] [G Loss: 5.083304]\n",
      "174 [D Loss: 0.023694, Acc.: 100.00%] [G Loss: 4.783118]\n",
      "175 [D Loss: 0.027254, Acc.: 100.00%] [G Loss: 5.008484]\n",
      "176 [D Loss: 0.008933, Acc.: 100.00%] [G Loss: 5.061911]\n",
      "177 [D Loss: 0.034005, Acc.: 100.00%] [G Loss: 5.585630]\n",
      "178 [D Loss: 0.060733, Acc.: 96.88%] [G Loss: 4.785553]\n",
      "179 [D Loss: 0.028335, Acc.: 100.00%] [G Loss: 4.845488]\n",
      "180 [D Loss: 0.020224, Acc.: 100.00%] [G Loss: 5.004276]\n",
      "181 [D Loss: 0.043181, Acc.: 100.00%] [G Loss: 5.529734]\n",
      "182 [D Loss: 0.089794, Acc.: 96.88%] [G Loss: 4.640406]\n",
      "183 [D Loss: 0.043877, Acc.: 100.00%] [G Loss: 5.129527]\n",
      "184 [D Loss: 0.036475, Acc.: 96.88%] [G Loss: 5.544619]\n",
      "185 [D Loss: 0.071147, Acc.: 100.00%] [G Loss: 4.411855]\n",
      "186 [D Loss: 0.065078, Acc.: 96.88%] [G Loss: 4.458317]\n",
      "187 [D Loss: 0.026561, Acc.: 100.00%] [G Loss: 5.120803]\n",
      "188 [D Loss: 0.027773, Acc.: 100.00%] [G Loss: 4.839694]\n",
      "189 [D Loss: 0.018167, Acc.: 100.00%] [G Loss: 4.830044]\n",
      "190 [D Loss: 0.061990, Acc.: 96.88%] [G Loss: 5.411364]\n",
      "191 [D Loss: 0.154187, Acc.: 93.75%] [G Loss: 4.703397]\n",
      "192 [D Loss: 0.061974, Acc.: 96.88%] [G Loss: 5.326181]\n",
      "193 [D Loss: 0.086374, Acc.: 96.88%] [G Loss: 4.169575]\n",
      "194 [D Loss: 0.028937, Acc.: 100.00%] [G Loss: 4.973838]\n",
      "195 [D Loss: 0.025254, Acc.: 100.00%] [G Loss: 5.213806]\n",
      "196 [D Loss: 0.399368, Acc.: 81.25%] [G Loss: 4.230125]\n",
      "197 [D Loss: 0.044207, Acc.: 100.00%] [G Loss: 5.522662]\n",
      "198 [D Loss: 0.389208, Acc.: 84.38%] [G Loss: 3.493410]\n",
      "199 [D Loss: 0.157545, Acc.: 93.75%] [G Loss: 4.412170]\n",
      "200 [D Loss: 0.009061, Acc.: 100.00%] [G Loss: 4.899890]\n",
      "201 [D Loss: 0.044353, Acc.: 96.88%] [G Loss: 4.832005]\n",
      "202 [D Loss: 0.047760, Acc.: 96.88%] [G Loss: 4.942671]\n",
      "203 [D Loss: 0.074413, Acc.: 96.88%] [G Loss: 4.599500]\n",
      "204 [D Loss: 0.051909, Acc.: 100.00%] [G Loss: 4.756691]\n",
      "205 [D Loss: 0.110691, Acc.: 93.75%] [G Loss: 4.924044]\n",
      "206 [D Loss: 0.228041, Acc.: 93.75%] [G Loss: 3.910275]\n",
      "207 [D Loss: 0.019998, Acc.: 100.00%] [G Loss: 3.972798]\n",
      "208 [D Loss: 0.089195, Acc.: 96.88%] [G Loss: 4.693882]\n",
      "209 [D Loss: 0.078494, Acc.: 100.00%] [G Loss: 3.771837]\n",
      "210 [D Loss: 0.166217, Acc.: 87.50%] [G Loss: 4.800925]\n",
      "211 [D Loss: 0.038043, Acc.: 100.00%] [G Loss: 4.842083]\n",
      "212 [D Loss: 0.228626, Acc.: 87.50%] [G Loss: 3.872572]\n",
      "213 [D Loss: 0.026427, Acc.: 100.00%] [G Loss: 4.214928]\n",
      "214 [D Loss: 0.053964, Acc.: 96.88%] [G Loss: 4.593996]\n",
      "215 [D Loss: 0.076466, Acc.: 96.88%] [G Loss: 4.349380]\n",
      "216 [D Loss: 0.050567, Acc.: 100.00%] [G Loss: 3.344311]\n",
      "217 [D Loss: 0.134839, Acc.: 93.75%] [G Loss: 3.983901]\n",
      "218 [D Loss: 0.022656, Acc.: 100.00%] [G Loss: 4.680681]\n",
      "219 [D Loss: 0.140261, Acc.: 100.00%] [G Loss: 4.050015]\n",
      "220 [D Loss: 0.150181, Acc.: 93.75%] [G Loss: 5.161615]\n",
      "221 [D Loss: 0.117384, Acc.: 93.75%] [G Loss: 3.865725]\n",
      "222 [D Loss: 0.087991, Acc.: 93.75%] [G Loss: 4.108633]\n",
      "223 [D Loss: 0.105590, Acc.: 93.75%] [G Loss: 4.938913]\n",
      "224 [D Loss: 0.780608, Acc.: 65.62%] [G Loss: 2.529140]\n",
      "225 [D Loss: 0.302572, Acc.: 87.50%] [G Loss: 2.727061]\n",
      "226 [D Loss: 0.040183, Acc.: 100.00%] [G Loss: 3.637424]\n",
      "227 [D Loss: 0.029140, Acc.: 100.00%] [G Loss: 4.068451]\n",
      "228 [D Loss: 0.110479, Acc.: 93.75%] [G Loss: 4.075694]\n",
      "229 [D Loss: 0.189087, Acc.: 87.50%] [G Loss: 3.620793]\n",
      "230 [D Loss: 0.072820, Acc.: 100.00%] [G Loss: 4.658399]\n",
      "231 [D Loss: 0.074355, Acc.: 100.00%] [G Loss: 4.177726]\n",
      "232 [D Loss: 0.037563, Acc.: 100.00%] [G Loss: 3.930807]\n",
      "233 [D Loss: 0.069692, Acc.: 100.00%] [G Loss: 4.273170]\n",
      "234 [D Loss: 0.220149, Acc.: 93.75%] [G Loss: 3.464387]\n",
      "235 [D Loss: 0.149046, Acc.: 90.62%] [G Loss: 4.526554]\n",
      "236 [D Loss: 0.153650, Acc.: 93.75%] [G Loss: 4.598064]\n",
      "237 [D Loss: 0.568220, Acc.: 68.75%] [G Loss: 3.078013]\n",
      "238 [D Loss: 0.089992, Acc.: 96.88%] [G Loss: 3.483614]\n",
      "239 [D Loss: 0.082387, Acc.: 96.88%] [G Loss: 4.565154]\n",
      "240 [D Loss: 0.635876, Acc.: 68.75%] [G Loss: 2.190639]\n",
      "241 [D Loss: 0.258879, Acc.: 87.50%] [G Loss: 2.910873]\n",
      "242 [D Loss: 0.077767, Acc.: 100.00%] [G Loss: 3.938261]\n",
      "243 [D Loss: 0.359185, Acc.: 78.12%] [G Loss: 3.991493]\n",
      "244 [D Loss: 0.037129, Acc.: 100.00%] [G Loss: 4.410620]\n",
      "245 [D Loss: 0.190837, Acc.: 93.75%] [G Loss: 3.899403]\n",
      "246 [D Loss: 0.047534, Acc.: 96.88%] [G Loss: 4.269618]\n",
      "247 [D Loss: 0.188580, Acc.: 90.62%] [G Loss: 4.375487]\n",
      "248 [D Loss: 0.098461, Acc.: 100.00%] [G Loss: 4.062551]\n",
      "249 [D Loss: 0.345932, Acc.: 90.62%] [G Loss: 3.424130]\n",
      "250 [D Loss: 0.050481, Acc.: 100.00%] [G Loss: 3.826633]\n",
      "251 [D Loss: 0.090586, Acc.: 100.00%] [G Loss: 3.673091]\n",
      "252 [D Loss: 0.080519, Acc.: 96.88%] [G Loss: 3.380823]\n",
      "253 [D Loss: 0.097058, Acc.: 96.88%] [G Loss: 3.314288]\n",
      "254 [D Loss: 0.128295, Acc.: 96.88%] [G Loss: 3.990681]\n",
      "255 [D Loss: 0.335905, Acc.: 81.25%] [G Loss: 2.618106]\n",
      "256 [D Loss: 0.258039, Acc.: 84.38%] [G Loss: 3.629718]\n",
      "257 [D Loss: 0.093693, Acc.: 100.00%] [G Loss: 4.581700]\n",
      "258 [D Loss: 0.173936, Acc.: 90.62%] [G Loss: 3.308075]\n",
      "259 [D Loss: 0.104413, Acc.: 96.88%] [G Loss: 4.406322]\n",
      "260 [D Loss: 0.150039, Acc.: 96.88%] [G Loss: 4.640788]\n",
      "261 [D Loss: 0.254274, Acc.: 87.50%] [G Loss: 2.881470]\n",
      "262 [D Loss: 0.292227, Acc.: 87.50%] [G Loss: 3.861637]\n",
      "263 [D Loss: 0.051282, Acc.: 100.00%] [G Loss: 4.604137]\n",
      "264 [D Loss: 0.342751, Acc.: 84.38%] [G Loss: 2.662616]\n",
      "265 [D Loss: 0.085957, Acc.: 96.88%] [G Loss: 3.973326]\n",
      "266 [D Loss: 0.114643, Acc.: 96.88%] [G Loss: 4.117047]\n",
      "267 [D Loss: 0.107356, Acc.: 96.88%] [G Loss: 3.375040]\n",
      "268 [D Loss: 0.121292, Acc.: 93.75%] [G Loss: 4.571232]\n",
      "269 [D Loss: 1.242364, Acc.: 59.38%] [G Loss: 1.492107]\n",
      "270 [D Loss: 0.587012, Acc.: 81.25%] [G Loss: 1.722293]\n",
      "271 [D Loss: 0.157874, Acc.: 93.75%] [G Loss: 3.408170]\n",
      "272 [D Loss: 0.045033, Acc.: 100.00%] [G Loss: 4.129050]\n",
      "273 [D Loss: 0.088032, Acc.: 100.00%] [G Loss: 3.716818]\n",
      "274 [D Loss: 0.087469, Acc.: 96.88%] [G Loss: 3.845556]\n",
      "275 [D Loss: 0.114416, Acc.: 93.75%] [G Loss: 4.474632]\n",
      "276 [D Loss: 0.073729, Acc.: 100.00%] [G Loss: 3.890316]\n",
      "277 [D Loss: 0.100336, Acc.: 100.00%] [G Loss: 3.549657]\n",
      "278 [D Loss: 0.090913, Acc.: 100.00%] [G Loss: 3.934296]\n",
      "279 [D Loss: 0.199976, Acc.: 93.75%] [G Loss: 3.959875]\n",
      "280 [D Loss: 0.088654, Acc.: 100.00%] [G Loss: 3.051619]\n",
      "281 [D Loss: 0.094401, Acc.: 93.75%] [G Loss: 3.477632]\n",
      "282 [D Loss: 0.087430, Acc.: 100.00%] [G Loss: 3.679871]\n",
      "283 [D Loss: 0.268985, Acc.: 84.38%] [G Loss: 3.227563]\n",
      "284 [D Loss: 0.117809, Acc.: 96.88%] [G Loss: 3.933128]\n",
      "285 [D Loss: 0.217194, Acc.: 87.50%] [G Loss: 3.042175]\n",
      "286 [D Loss: 0.172755, Acc.: 93.75%] [G Loss: 4.582018]\n",
      "287 [D Loss: 0.362360, Acc.: 87.50%] [G Loss: 2.187980]\n",
      "288 [D Loss: 0.198558, Acc.: 90.62%] [G Loss: 3.492389]\n",
      "289 [D Loss: 0.068143, Acc.: 100.00%] [G Loss: 5.096503]\n",
      "290 [D Loss: 0.581202, Acc.: 75.00%] [G Loss: 2.225357]\n",
      "291 [D Loss: 0.339377, Acc.: 81.25%] [G Loss: 4.045353]\n",
      "292 [D Loss: 0.050984, Acc.: 100.00%] [G Loss: 4.814508]\n",
      "293 [D Loss: 0.251969, Acc.: 93.75%] [G Loss: 3.115111]\n",
      "294 [D Loss: 0.196884, Acc.: 90.62%] [G Loss: 4.710396]\n",
      "295 [D Loss: 0.307620, Acc.: 93.75%] [G Loss: 3.958224]\n",
      "296 [D Loss: 0.071962, Acc.: 100.00%] [G Loss: 3.620336]\n",
      "297 [D Loss: 0.103785, Acc.: 96.88%] [G Loss: 4.142329]\n",
      "298 [D Loss: 0.250723, Acc.: 90.62%] [G Loss: 3.130683]\n",
      "299 [D Loss: 0.097271, Acc.: 96.88%] [G Loss: 3.496122]\n",
      "300 [D Loss: 0.416527, Acc.: 71.88%] [G Loss: 3.673040]\n",
      "301 [D Loss: 0.196760, Acc.: 93.75%] [G Loss: 4.054636]\n",
      "302 [D Loss: 0.292482, Acc.: 84.38%] [G Loss: 3.965884]\n",
      "303 [D Loss: 0.234396, Acc.: 93.75%] [G Loss: 4.076530]\n",
      "304 [D Loss: 0.558426, Acc.: 68.75%] [G Loss: 1.777553]\n",
      "305 [D Loss: 0.264728, Acc.: 87.50%] [G Loss: 3.549247]\n",
      "306 [D Loss: 0.038517, Acc.: 100.00%] [G Loss: 4.980771]\n",
      "307 [D Loss: 0.251162, Acc.: 84.38%] [G Loss: 2.607308]\n",
      "308 [D Loss: 0.281456, Acc.: 90.62%] [G Loss: 3.951413]\n",
      "309 [D Loss: 0.229216, Acc.: 96.88%] [G Loss: 3.232911]\n",
      "310 [D Loss: 0.078248, Acc.: 100.00%] [G Loss: 3.475179]\n",
      "311 [D Loss: 0.171596, Acc.: 96.88%] [G Loss: 3.144830]\n",
      "312 [D Loss: 0.147927, Acc.: 96.88%] [G Loss: 3.782768]\n",
      "313 [D Loss: 0.399141, Acc.: 81.25%] [G Loss: 3.397606]\n",
      "314 [D Loss: 0.297813, Acc.: 84.38%] [G Loss: 5.359223]\n",
      "315 [D Loss: 1.420755, Acc.: 43.75%] [G Loss: 1.820371]\n",
      "316 [D Loss: 0.599643, Acc.: 65.62%] [G Loss: 1.752798]\n",
      "317 [D Loss: 0.211423, Acc.: 87.50%] [G Loss: 3.520841]\n",
      "318 [D Loss: 0.029378, Acc.: 100.00%] [G Loss: 4.763462]\n",
      "319 [D Loss: 0.227994, Acc.: 90.62%] [G Loss: 2.618242]\n",
      "320 [D Loss: 0.153411, Acc.: 96.88%] [G Loss: 2.990632]\n",
      "321 [D Loss: 0.054719, Acc.: 100.00%] [G Loss: 3.850866]\n",
      "322 [D Loss: 0.076962, Acc.: 96.88%] [G Loss: 4.230186]\n",
      "323 [D Loss: 0.251593, Acc.: 87.50%] [G Loss: 2.624793]\n",
      "324 [D Loss: 0.163726, Acc.: 93.75%] [G Loss: 3.594185]\n",
      "325 [D Loss: 0.067912, Acc.: 96.88%] [G Loss: 4.221861]\n",
      "326 [D Loss: 0.234126, Acc.: 90.62%] [G Loss: 2.997817]\n",
      "327 [D Loss: 0.086680, Acc.: 96.88%] [G Loss: 3.816154]\n",
      "328 [D Loss: 0.143469, Acc.: 100.00%] [G Loss: 3.691124]\n",
      "329 [D Loss: 0.120878, Acc.: 96.88%] [G Loss: 3.702268]\n",
      "330 [D Loss: 0.372838, Acc.: 84.38%] [G Loss: 3.292644]\n",
      "331 [D Loss: 0.109740, Acc.: 96.88%] [G Loss: 3.655671]\n",
      "332 [D Loss: 0.439662, Acc.: 75.00%] [G Loss: 2.195800]\n",
      "333 [D Loss: 0.147143, Acc.: 93.75%] [G Loss: 2.904711]\n",
      "334 [D Loss: 0.143201, Acc.: 100.00%] [G Loss: 4.715859]\n",
      "335 [D Loss: 1.370854, Acc.: 46.88%] [G Loss: 1.264538]\n",
      "336 [D Loss: 0.354806, Acc.: 87.50%] [G Loss: 2.589959]\n",
      "337 [D Loss: 0.128349, Acc.: 100.00%] [G Loss: 3.371313]\n",
      "338 [D Loss: 0.386361, Acc.: 78.12%] [G Loss: 2.778731]\n",
      "339 [D Loss: 0.196467, Acc.: 100.00%] [G Loss: 3.338481]\n",
      "340 [D Loss: 0.122200, Acc.: 100.00%] [G Loss: 2.963377]\n",
      "341 [D Loss: 0.199277, Acc.: 96.88%] [G Loss: 2.607655]\n",
      "342 [D Loss: 0.377176, Acc.: 78.12%] [G Loss: 2.838595]\n",
      "343 [D Loss: 0.159658, Acc.: 96.88%] [G Loss: 3.697546]\n",
      "344 [D Loss: 0.750727, Acc.: 62.50%] [G Loss: 1.919703]\n",
      "345 [D Loss: 0.130886, Acc.: 96.88%] [G Loss: 3.262327]\n",
      "346 [D Loss: 0.128853, Acc.: 96.88%] [G Loss: 3.292677]\n",
      "347 [D Loss: 0.118087, Acc.: 100.00%] [G Loss: 3.364047]\n",
      "348 [D Loss: 0.298615, Acc.: 87.50%] [G Loss: 2.852886]\n",
      "349 [D Loss: 0.204656, Acc.: 93.75%] [G Loss: 4.008920]\n",
      "350 [D Loss: 0.598821, Acc.: 68.75%] [G Loss: 2.251028]\n",
      "351 [D Loss: 0.104146, Acc.: 96.88%] [G Loss: 3.343007]\n",
      "352 [D Loss: 1.050628, Acc.: 31.25%] [G Loss: 1.273472]\n",
      "353 [D Loss: 0.380400, Acc.: 75.00%] [G Loss: 2.624859]\n",
      "354 [D Loss: 0.039796, Acc.: 100.00%] [G Loss: 4.456549]\n",
      "355 [D Loss: 0.193658, Acc.: 96.88%] [G Loss: 3.076193]\n",
      "356 [D Loss: 0.127497, Acc.: 100.00%] [G Loss: 2.934072]\n",
      "357 [D Loss: 0.209743, Acc.: 90.62%] [G Loss: 3.551094]\n",
      "358 [D Loss: 0.549622, Acc.: 71.88%] [G Loss: 2.505565]\n",
      "359 [D Loss: 0.363398, Acc.: 90.62%] [G Loss: 3.309274]\n",
      "360 [D Loss: 0.467554, Acc.: 71.88%] [G Loss: 2.503584]\n",
      "361 [D Loss: 0.288448, Acc.: 84.38%] [G Loss: 3.344911]\n",
      "362 [D Loss: 0.995618, Acc.: 62.50%] [G Loss: 2.140098]\n",
      "363 [D Loss: 0.124520, Acc.: 96.88%] [G Loss: 2.987590]\n",
      "364 [D Loss: 0.301620, Acc.: 90.62%] [G Loss: 3.000845]\n",
      "365 [D Loss: 0.325644, Acc.: 93.75%] [G Loss: 2.565623]\n",
      "366 [D Loss: 0.238284, Acc.: 93.75%] [G Loss: 2.516242]\n",
      "367 [D Loss: 0.388839, Acc.: 78.12%] [G Loss: 3.503406]\n",
      "368 [D Loss: 0.521166, Acc.: 78.12%] [G Loss: 1.815556]\n",
      "369 [D Loss: 0.191437, Acc.: 93.75%] [G Loss: 2.881200]\n",
      "370 [D Loss: 0.449903, Acc.: 75.00%] [G Loss: 2.563743]\n",
      "371 [D Loss: 0.356487, Acc.: 87.50%] [G Loss: 2.101024]\n",
      "372 [D Loss: 0.145666, Acc.: 100.00%] [G Loss: 2.696407]\n",
      "373 [D Loss: 0.314671, Acc.: 84.38%] [G Loss: 1.925185]\n",
      "374 [D Loss: 0.207666, Acc.: 87.50%] [G Loss: 3.456791]\n",
      "375 [D Loss: 1.141611, Acc.: 40.62%] [G Loss: 0.647516]\n",
      "376 [D Loss: 0.657988, Acc.: 65.62%] [G Loss: 2.642067]\n",
      "377 [D Loss: 0.144643, Acc.: 96.88%] [G Loss: 3.673913]\n",
      "378 [D Loss: 0.507496, Acc.: 71.88%] [G Loss: 2.073380]\n",
      "379 [D Loss: 0.260557, Acc.: 90.62%] [G Loss: 2.767248]\n",
      "380 [D Loss: 0.229757, Acc.: 90.62%] [G Loss: 3.550966]\n",
      "381 [D Loss: 0.420398, Acc.: 78.12%] [G Loss: 2.945837]\n",
      "382 [D Loss: 0.140282, Acc.: 100.00%] [G Loss: 3.108512]\n",
      "383 [D Loss: 0.162046, Acc.: 100.00%] [G Loss: 3.134789]\n",
      "384 [D Loss: 0.469056, Acc.: 81.25%] [G Loss: 1.878281]\n",
      "385 [D Loss: 0.113926, Acc.: 96.88%] [G Loss: 3.001920]\n",
      "386 [D Loss: 0.423321, Acc.: 81.25%] [G Loss: 3.057088]\n",
      "387 [D Loss: 0.429578, Acc.: 78.12%] [G Loss: 2.899128]\n",
      "388 [D Loss: 0.644537, Acc.: 56.25%] [G Loss: 1.725441]\n",
      "389 [D Loss: 0.320755, Acc.: 87.50%] [G Loss: 3.420241]\n",
      "390 [D Loss: 0.652584, Acc.: 65.62%] [G Loss: 1.914843]\n",
      "391 [D Loss: 0.285144, Acc.: 90.62%] [G Loss: 2.686445]\n",
      "392 [D Loss: 0.478778, Acc.: 71.88%] [G Loss: 2.752162]\n",
      "393 [D Loss: 0.216220, Acc.: 93.75%] [G Loss: 3.409545]\n",
      "394 [D Loss: 0.947210, Acc.: 46.88%] [G Loss: 1.442894]\n",
      "395 [D Loss: 0.278966, Acc.: 87.50%] [G Loss: 3.647856]\n",
      "396 [D Loss: 0.839618, Acc.: 50.00%] [G Loss: 1.258800]\n",
      "397 [D Loss: 0.298055, Acc.: 81.25%] [G Loss: 2.495278]\n",
      "398 [D Loss: 0.531906, Acc.: 75.00%] [G Loss: 2.863758]\n",
      "399 [D Loss: 0.501860, Acc.: 71.88%] [G Loss: 1.949056]\n",
      "400 [D Loss: 0.240482, Acc.: 90.62%] [G Loss: 2.689308]\n",
      "401 [D Loss: 0.755540, Acc.: 53.12%] [G Loss: 1.505804]\n",
      "402 [D Loss: 0.251628, Acc.: 90.62%] [G Loss: 2.748733]\n",
      "403 [D Loss: 0.436966, Acc.: 81.25%] [G Loss: 2.867455]\n",
      "404 [D Loss: 1.401030, Acc.: 25.00%] [G Loss: 0.621927]\n",
      "405 [D Loss: 0.499721, Acc.: 65.62%] [G Loss: 2.993242]\n",
      "406 [D Loss: 0.748534, Acc.: 56.25%] [G Loss: 1.187907]\n",
      "407 [D Loss: 0.411077, Acc.: 81.25%] [G Loss: 2.121677]\n",
      "408 [D Loss: 0.449758, Acc.: 81.25%] [G Loss: 2.068336]\n",
      "409 [D Loss: 0.289411, Acc.: 87.50%] [G Loss: 2.282841]\n",
      "410 [D Loss: 0.566659, Acc.: 65.62%] [G Loss: 1.450774]\n",
      "411 [D Loss: 0.663050, Acc.: 56.25%] [G Loss: 2.107498]\n",
      "412 [D Loss: 0.632125, Acc.: 65.62%] [G Loss: 1.727951]\n",
      "413 [D Loss: 0.638419, Acc.: 56.25%] [G Loss: 1.531475]\n",
      "414 [D Loss: 0.480036, Acc.: 78.12%] [G Loss: 1.686278]\n",
      "415 [D Loss: 0.365003, Acc.: 84.38%] [G Loss: 2.511395]\n",
      "416 [D Loss: 1.716886, Acc.: 15.62%] [G Loss: 0.380986]\n",
      "417 [D Loss: 0.726798, Acc.: 59.38%] [G Loss: 1.402297]\n",
      "418 [D Loss: 0.502436, Acc.: 78.12%] [G Loss: 1.645053]\n",
      "419 [D Loss: 0.953231, Acc.: 28.12%] [G Loss: 0.881721]\n",
      "420 [D Loss: 0.423902, Acc.: 84.38%] [G Loss: 1.640728]\n",
      "421 [D Loss: 0.928672, Acc.: 34.38%] [G Loss: 0.967680]\n",
      "422 [D Loss: 0.465170, Acc.: 71.88%] [G Loss: 1.262579]\n",
      "423 [D Loss: 0.920492, Acc.: 34.38%] [G Loss: 0.904505]\n",
      "424 [D Loss: 0.697904, Acc.: 56.25%] [G Loss: 1.198089]\n",
      "425 [D Loss: 0.678026, Acc.: 53.12%] [G Loss: 1.412087]\n",
      "426 [D Loss: 0.594296, Acc.: 65.62%] [G Loss: 1.299617]\n",
      "427 [D Loss: 0.581814, Acc.: 75.00%] [G Loss: 1.157443]\n",
      "428 [D Loss: 0.633927, Acc.: 59.38%] [G Loss: 1.187755]\n",
      "429 [D Loss: 0.715625, Acc.: 50.00%] [G Loss: 1.258383]\n",
      "430 [D Loss: 0.894925, Acc.: 50.00%] [G Loss: 0.968589]\n",
      "431 [D Loss: 0.534262, Acc.: 65.62%] [G Loss: 1.152445]\n",
      "432 [D Loss: 0.769003, Acc.: 43.75%] [G Loss: 0.914385]\n",
      "433 [D Loss: 0.589147, Acc.: 53.12%] [G Loss: 1.548279]\n",
      "434 [D Loss: 0.904754, Acc.: 40.62%] [G Loss: 0.830076]\n",
      "435 [D Loss: 0.834208, Acc.: 40.62%] [G Loss: 0.854988]\n",
      "436 [D Loss: 0.766662, Acc.: 50.00%] [G Loss: 1.004358]\n",
      "437 [D Loss: 0.679167, Acc.: 53.12%] [G Loss: 1.039264]\n",
      "438 [D Loss: 0.719979, Acc.: 53.12%] [G Loss: 0.983653]\n",
      "439 [D Loss: 0.661000, Acc.: 53.12%] [G Loss: 1.080048]\n",
      "440 [D Loss: 0.655108, Acc.: 59.38%] [G Loss: 1.079135]\n",
      "441 [D Loss: 0.930843, Acc.: 37.50%] [G Loss: 0.745727]\n",
      "442 [D Loss: 0.740867, Acc.: 50.00%] [G Loss: 0.851414]\n",
      "443 [D Loss: 0.633269, Acc.: 59.38%] [G Loss: 0.948768]\n",
      "444 [D Loss: 0.663359, Acc.: 46.88%] [G Loss: 0.888647]\n",
      "445 [D Loss: 0.670756, Acc.: 53.12%] [G Loss: 0.997052]\n",
      "446 [D Loss: 0.707585, Acc.: 46.88%] [G Loss: 0.883311]\n",
      "447 [D Loss: 0.556470, Acc.: 75.00%] [G Loss: 1.129747]\n",
      "448 [D Loss: 0.841383, Acc.: 28.12%] [G Loss: 0.841073]\n",
      "449 [D Loss: 0.839999, Acc.: 34.38%] [G Loss: 0.698977]\n",
      "450 [D Loss: 0.625544, Acc.: 56.25%] [G Loss: 0.959348]\n",
      "451 [D Loss: 0.569287, Acc.: 68.75%] [G Loss: 1.065012]\n",
      "452 [D Loss: 0.952663, Acc.: 28.12%] [G Loss: 0.674942]\n",
      "453 [D Loss: 0.683383, Acc.: 53.12%] [G Loss: 0.779846]\n",
      "454 [D Loss: 0.695466, Acc.: 46.88%] [G Loss: 0.970945]\n",
      "455 [D Loss: 0.723406, Acc.: 46.88%] [G Loss: 0.921049]\n",
      "456 [D Loss: 0.721895, Acc.: 50.00%] [G Loss: 0.862697]\n",
      "457 [D Loss: 0.693519, Acc.: 53.12%] [G Loss: 0.849349]\n",
      "458 [D Loss: 0.611648, Acc.: 59.38%] [G Loss: 0.943186]\n",
      "459 [D Loss: 0.712383, Acc.: 56.25%] [G Loss: 0.788089]\n",
      "460 [D Loss: 0.736105, Acc.: 43.75%] [G Loss: 0.698140]\n",
      "461 [D Loss: 0.833647, Acc.: 31.25%] [G Loss: 0.678619]\n",
      "462 [D Loss: 0.723988, Acc.: 40.62%] [G Loss: 0.736204]\n",
      "463 [D Loss: 0.665225, Acc.: 46.88%] [G Loss: 0.819545]\n",
      "464 [D Loss: 0.685199, Acc.: 50.00%] [G Loss: 0.836027]\n",
      "465 [D Loss: 0.676469, Acc.: 53.12%] [G Loss: 0.778120]\n",
      "466 [D Loss: 0.771393, Acc.: 43.75%] [G Loss: 0.804724]\n",
      "467 [D Loss: 0.735189, Acc.: 37.50%] [G Loss: 0.752354]\n",
      "468 [D Loss: 0.629557, Acc.: 56.25%] [G Loss: 0.894834]\n",
      "469 [D Loss: 0.642900, Acc.: 59.38%] [G Loss: 0.826646]\n",
      "470 [D Loss: 0.679482, Acc.: 56.25%] [G Loss: 0.812846]\n",
      "471 [D Loss: 0.738690, Acc.: 40.62%] [G Loss: 0.718909]\n",
      "472 [D Loss: 0.758850, Acc.: 43.75%] [G Loss: 0.670099]\n",
      "473 [D Loss: 0.688467, Acc.: 46.88%] [G Loss: 0.703899]\n",
      "474 [D Loss: 0.681819, Acc.: 50.00%] [G Loss: 0.753624]\n",
      "475 [D Loss: 0.739369, Acc.: 46.88%] [G Loss: 0.732405]\n",
      "476 [D Loss: 0.773873, Acc.: 37.50%] [G Loss: 0.672107]\n",
      "477 [D Loss: 0.694303, Acc.: 56.25%] [G Loss: 0.759753]\n",
      "478 [D Loss: 0.571647, Acc.: 65.62%] [G Loss: 0.875050]\n",
      "479 [D Loss: 0.763325, Acc.: 34.38%] [G Loss: 0.790013]\n",
      "480 [D Loss: 0.691540, Acc.: 53.12%] [G Loss: 0.728788]\n",
      "481 [D Loss: 0.682950, Acc.: 40.62%] [G Loss: 0.766819]\n",
      "482 [D Loss: 0.675138, Acc.: 50.00%] [G Loss: 0.738415]\n",
      "483 [D Loss: 0.737894, Acc.: 40.62%] [G Loss: 0.675826]\n",
      "484 [D Loss: 0.695596, Acc.: 43.75%] [G Loss: 0.750896]\n",
      "485 [D Loss: 0.640557, Acc.: 50.00%] [G Loss: 0.726764]\n",
      "486 [D Loss: 0.705339, Acc.: 40.62%] [G Loss: 0.706998]\n",
      "487 [D Loss: 0.746266, Acc.: 37.50%] [G Loss: 0.686093]\n",
      "488 [D Loss: 0.682348, Acc.: 40.62%] [G Loss: 0.679498]\n",
      "489 [D Loss: 0.713972, Acc.: 40.62%] [G Loss: 0.704840]\n",
      "490 [D Loss: 0.693906, Acc.: 43.75%] [G Loss: 0.720735]\n",
      "491 [D Loss: 0.729075, Acc.: 40.62%] [G Loss: 0.691027]\n",
      "492 [D Loss: 0.642610, Acc.: 53.12%] [G Loss: 0.719664]\n",
      "493 [D Loss: 0.661789, Acc.: 46.88%] [G Loss: 0.694406]\n",
      "494 [D Loss: 0.697813, Acc.: 37.50%] [G Loss: 0.691647]\n",
      "495 [D Loss: 0.675167, Acc.: 43.75%] [G Loss: 0.699285]\n",
      "496 [D Loss: 0.766074, Acc.: 34.38%] [G Loss: 0.669577]\n",
      "497 [D Loss: 0.687685, Acc.: 50.00%] [G Loss: 0.688043]\n",
      "498 [D Loss: 0.658495, Acc.: 50.00%] [G Loss: 0.777622]\n",
      "499 [D Loss: 0.654204, Acc.: 43.75%] [G Loss: 0.797916]\n",
      "500 [D Loss: 0.690751, Acc.: 46.88%] [G Loss: 0.734486]\n",
      "501 [D Loss: 0.749317, Acc.: 37.50%] [G Loss: 0.682612]\n",
      "502 [D Loss: 0.709183, Acc.: 40.62%] [G Loss: 0.693273]\n",
      "503 [D Loss: 0.677490, Acc.: 56.25%] [G Loss: 0.672577]\n",
      "504 [D Loss: 0.702179, Acc.: 43.75%] [G Loss: 0.685406]\n",
      "505 [D Loss: 0.640201, Acc.: 56.25%] [G Loss: 0.730503]\n",
      "506 [D Loss: 0.730453, Acc.: 40.62%] [G Loss: 0.672843]\n",
      "507 [D Loss: 0.672759, Acc.: 43.75%] [G Loss: 0.672572]\n",
      "508 [D Loss: 0.711599, Acc.: 40.62%] [G Loss: 0.670884]\n",
      "509 [D Loss: 0.668218, Acc.: 50.00%] [G Loss: 0.671296]\n",
      "510 [D Loss: 0.639418, Acc.: 50.00%] [G Loss: 0.737085]\n",
      "511 [D Loss: 0.686570, Acc.: 40.62%] [G Loss: 0.724070]\n",
      "512 [D Loss: 0.636552, Acc.: 43.75%] [G Loss: 0.701347]\n",
      "513 [D Loss: 0.627501, Acc.: 59.38%] [G Loss: 0.728835]\n",
      "514 [D Loss: 0.609739, Acc.: 56.25%] [G Loss: 0.788514]\n",
      "515 [D Loss: 0.652052, Acc.: 50.00%] [G Loss: 0.749477]\n",
      "516 [D Loss: 0.657458, Acc.: 56.25%] [G Loss: 0.780470]\n",
      "517 [D Loss: 0.721347, Acc.: 34.38%] [G Loss: 0.719381]\n",
      "518 [D Loss: 0.698588, Acc.: 40.62%] [G Loss: 0.696908]\n",
      "519 [D Loss: 0.604619, Acc.: 53.12%] [G Loss: 0.703005]\n",
      "520 [D Loss: 0.646665, Acc.: 50.00%] [G Loss: 0.687921]\n",
      "521 [D Loss: 0.665026, Acc.: 46.88%] [G Loss: 0.693760]\n",
      "522 [D Loss: 0.718473, Acc.: 46.88%] [G Loss: 0.684331]\n",
      "523 [D Loss: 0.660797, Acc.: 43.75%] [G Loss: 0.719604]\n",
      "524 [D Loss: 0.711959, Acc.: 50.00%] [G Loss: 0.718197]\n",
      "525 [D Loss: 0.661177, Acc.: 50.00%] [G Loss: 0.690573]\n",
      "526 [D Loss: 0.609532, Acc.: 50.00%] [G Loss: 0.760611]\n",
      "527 [D Loss: 0.654346, Acc.: 46.88%] [G Loss: 0.752591]\n",
      "528 [D Loss: 0.632080, Acc.: 53.12%] [G Loss: 0.770539]\n",
      "529 [D Loss: 0.658133, Acc.: 46.88%] [G Loss: 0.721158]\n",
      "530 [D Loss: 0.654474, Acc.: 50.00%] [G Loss: 0.733170]\n",
      "531 [D Loss: 0.652932, Acc.: 46.88%] [G Loss: 0.741361]\n",
      "532 [D Loss: 0.682182, Acc.: 46.88%] [G Loss: 0.748739]\n",
      "533 [D Loss: 0.653013, Acc.: 50.00%] [G Loss: 0.705677]\n",
      "534 [D Loss: 0.606966, Acc.: 56.25%] [G Loss: 0.744499]\n",
      "535 [D Loss: 0.670168, Acc.: 50.00%] [G Loss: 0.757480]\n",
      "536 [D Loss: 0.761345, Acc.: 28.12%] [G Loss: 0.681127]\n",
      "537 [D Loss: 0.666276, Acc.: 40.62%] [G Loss: 0.678984]\n",
      "538 [D Loss: 0.673711, Acc.: 53.12%] [G Loss: 0.711046]\n",
      "539 [D Loss: 0.655581, Acc.: 53.12%] [G Loss: 0.709755]\n",
      "540 [D Loss: 0.666655, Acc.: 53.12%] [G Loss: 0.715886]\n",
      "541 [D Loss: 0.654213, Acc.: 46.88%] [G Loss: 0.691723]\n",
      "542 [D Loss: 0.649117, Acc.: 50.00%] [G Loss: 0.727546]\n",
      "543 [D Loss: 0.641666, Acc.: 56.25%] [G Loss: 0.717656]\n",
      "544 [D Loss: 0.676626, Acc.: 40.62%] [G Loss: 0.684958]\n",
      "545 [D Loss: 0.711922, Acc.: 40.62%] [G Loss: 0.675113]\n",
      "546 [D Loss: 0.620734, Acc.: 53.12%] [G Loss: 0.675155]\n",
      "547 [D Loss: 0.646844, Acc.: 50.00%] [G Loss: 0.711885]\n",
      "548 [D Loss: 0.646838, Acc.: 56.25%] [G Loss: 0.714778]\n",
      "549 [D Loss: 0.626571, Acc.: 56.25%] [G Loss: 0.717380]\n",
      "550 [D Loss: 0.681419, Acc.: 37.50%] [G Loss: 0.682987]\n",
      "551 [D Loss: 0.694521, Acc.: 46.88%] [G Loss: 0.733119]\n",
      "552 [D Loss: 0.681870, Acc.: 43.75%] [G Loss: 0.692690]\n",
      "553 [D Loss: 0.697657, Acc.: 37.50%] [G Loss: 0.686751]\n",
      "554 [D Loss: 0.692731, Acc.: 46.88%] [G Loss: 0.698085]\n",
      "555 [D Loss: 0.684945, Acc.: 43.75%] [G Loss: 0.665347]\n",
      "556 [D Loss: 0.659572, Acc.: 53.12%] [G Loss: 0.662890]\n",
      "557 [D Loss: 0.678139, Acc.: 53.12%] [G Loss: 0.690974]\n",
      "558 [D Loss: 0.632300, Acc.: 50.00%] [G Loss: 0.680140]\n",
      "559 [D Loss: 0.673549, Acc.: 53.12%] [G Loss: 0.727886]\n",
      "560 [D Loss: 0.701742, Acc.: 50.00%] [G Loss: 0.771930]\n",
      "561 [D Loss: 0.654360, Acc.: 68.75%] [G Loss: 0.739668]\n",
      "562 [D Loss: 0.680080, Acc.: 50.00%] [G Loss: 0.698719]\n",
      "563 [D Loss: 0.633783, Acc.: 62.50%] [G Loss: 0.687847]\n",
      "564 [D Loss: 0.685439, Acc.: 53.12%] [G Loss: 0.670645]\n",
      "565 [D Loss: 0.655519, Acc.: 43.75%] [G Loss: 0.653013]\n",
      "566 [D Loss: 0.669814, Acc.: 43.75%] [G Loss: 0.678276]\n",
      "567 [D Loss: 0.687476, Acc.: 46.88%] [G Loss: 0.731218]\n",
      "568 [D Loss: 0.643543, Acc.: 59.38%] [G Loss: 0.693994]\n",
      "569 [D Loss: 0.660781, Acc.: 53.12%] [G Loss: 0.728152]\n",
      "570 [D Loss: 0.647313, Acc.: 56.25%] [G Loss: 0.730454]\n",
      "571 [D Loss: 0.694786, Acc.: 46.88%] [G Loss: 0.680954]\n",
      "572 [D Loss: 0.662150, Acc.: 46.88%] [G Loss: 0.641252]\n",
      "573 [D Loss: 0.666261, Acc.: 53.12%] [G Loss: 0.605841]\n",
      "574 [D Loss: 0.671971, Acc.: 40.62%] [G Loss: 0.625972]\n",
      "575 [D Loss: 0.658724, Acc.: 50.00%] [G Loss: 0.646999]\n",
      "576 [D Loss: 0.651109, Acc.: 56.25%] [G Loss: 0.644565]\n",
      "577 [D Loss: 0.650357, Acc.: 65.62%] [G Loss: 0.651443]\n",
      "578 [D Loss: 0.668589, Acc.: 53.12%] [G Loss: 0.651998]\n",
      "579 [D Loss: 0.696877, Acc.: 46.88%] [G Loss: 0.665476]\n",
      "580 [D Loss: 0.645587, Acc.: 53.12%] [G Loss: 0.691407]\n",
      "581 [D Loss: 0.702310, Acc.: 46.88%] [G Loss: 0.744790]\n",
      "582 [D Loss: 0.643026, Acc.: 53.12%] [G Loss: 0.719230]\n",
      "583 [D Loss: 0.697286, Acc.: 40.62%] [G Loss: 0.734051]\n",
      "584 [D Loss: 0.646640, Acc.: 56.25%] [G Loss: 0.720670]\n",
      "585 [D Loss: 0.647861, Acc.: 56.25%] [G Loss: 0.742098]\n",
      "586 [D Loss: 0.688572, Acc.: 46.88%] [G Loss: 0.725165]\n",
      "587 [D Loss: 0.678644, Acc.: 43.75%] [G Loss: 0.675699]\n",
      "588 [D Loss: 0.619884, Acc.: 53.12%] [G Loss: 0.682086]\n",
      "589 [D Loss: 0.638516, Acc.: 56.25%] [G Loss: 0.715052]\n",
      "590 [D Loss: 0.671400, Acc.: 40.62%] [G Loss: 0.727960]\n",
      "591 [D Loss: 0.668315, Acc.: 53.12%] [G Loss: 0.704353]\n",
      "592 [D Loss: 0.640881, Acc.: 53.12%] [G Loss: 0.706604]\n",
      "593 [D Loss: 0.645031, Acc.: 50.00%] [G Loss: 0.711649]\n",
      "594 [D Loss: 0.653471, Acc.: 50.00%] [G Loss: 0.707183]\n",
      "595 [D Loss: 0.650411, Acc.: 50.00%] [G Loss: 0.713627]\n",
      "596 [D Loss: 0.660270, Acc.: 50.00%] [G Loss: 0.698020]\n",
      "597 [D Loss: 0.641189, Acc.: 53.12%] [G Loss: 0.676921]\n",
      "598 [D Loss: 0.669326, Acc.: 50.00%] [G Loss: 0.683827]\n",
      "599 [D Loss: 0.646944, Acc.: 56.25%] [G Loss: 0.688116]\n",
      "600 [D Loss: 0.648731, Acc.: 56.25%] [G Loss: 0.668971]\n",
      "601 [D Loss: 0.641126, Acc.: 53.12%] [G Loss: 0.681520]\n",
      "602 [D Loss: 0.658821, Acc.: 56.25%] [G Loss: 0.725580]\n",
      "603 [D Loss: 0.638487, Acc.: 59.38%] [G Loss: 0.728675]\n",
      "604 [D Loss: 0.636935, Acc.: 62.50%] [G Loss: 0.734117]\n",
      "605 [D Loss: 0.667471, Acc.: 46.88%] [G Loss: 0.711116]\n",
      "606 [D Loss: 0.641448, Acc.: 59.38%] [G Loss: 0.729509]\n",
      "607 [D Loss: 0.645038, Acc.: 56.25%] [G Loss: 0.678745]\n",
      "608 [D Loss: 0.656412, Acc.: 59.38%] [G Loss: 0.682937]\n",
      "609 [D Loss: 0.629022, Acc.: 59.38%] [G Loss: 0.703327]\n",
      "610 [D Loss: 0.628136, Acc.: 62.50%] [G Loss: 0.738858]\n",
      "611 [D Loss: 0.637210, Acc.: 53.12%] [G Loss: 0.745998]\n",
      "612 [D Loss: 0.683717, Acc.: 50.00%] [G Loss: 0.749457]\n",
      "613 [D Loss: 0.686642, Acc.: 43.75%] [G Loss: 0.741849]\n",
      "614 [D Loss: 0.660224, Acc.: 56.25%] [G Loss: 0.744429]\n",
      "615 [D Loss: 0.642091, Acc.: 46.88%] [G Loss: 0.708369]\n",
      "616 [D Loss: 0.656725, Acc.: 50.00%] [G Loss: 0.727087]\n",
      "617 [D Loss: 0.663567, Acc.: 53.12%] [G Loss: 0.695576]\n",
      "618 [D Loss: 0.677985, Acc.: 46.88%] [G Loss: 0.709193]\n",
      "619 [D Loss: 0.616483, Acc.: 59.38%] [G Loss: 0.716536]\n",
      "620 [D Loss: 0.635927, Acc.: 46.88%] [G Loss: 0.746643]\n",
      "621 [D Loss: 0.670258, Acc.: 46.88%] [G Loss: 0.708621]\n",
      "622 [D Loss: 0.641152, Acc.: 59.38%] [G Loss: 0.712124]\n",
      "623 [D Loss: 0.664109, Acc.: 53.12%] [G Loss: 0.694069]\n",
      "624 [D Loss: 0.613921, Acc.: 50.00%] [G Loss: 0.694974]\n",
      "625 [D Loss: 0.658876, Acc.: 53.12%] [G Loss: 0.735777]\n",
      "626 [D Loss: 0.694597, Acc.: 53.12%] [G Loss: 0.717419]\n",
      "627 [D Loss: 0.610469, Acc.: 62.50%] [G Loss: 0.753035]\n",
      "628 [D Loss: 0.641166, Acc.: 59.38%] [G Loss: 0.745294]\n",
      "629 [D Loss: 0.670096, Acc.: 50.00%] [G Loss: 0.716382]\n",
      "630 [D Loss: 0.638382, Acc.: 50.00%] [G Loss: 0.689368]\n",
      "631 [D Loss: 0.638072, Acc.: 59.38%] [G Loss: 0.685070]\n",
      "632 [D Loss: 0.632404, Acc.: 68.75%] [G Loss: 0.688004]\n",
      "633 [D Loss: 0.652799, Acc.: 53.12%] [G Loss: 0.675234]\n",
      "634 [D Loss: 0.609577, Acc.: 68.75%] [G Loss: 0.709847]\n",
      "635 [D Loss: 0.622251, Acc.: 59.38%] [G Loss: 0.728033]\n",
      "636 [D Loss: 0.614629, Acc.: 65.62%] [G Loss: 0.740888]\n",
      "637 [D Loss: 0.648200, Acc.: 53.12%] [G Loss: 0.766023]\n",
      "638 [D Loss: 0.629738, Acc.: 65.62%] [G Loss: 0.746640]\n",
      "639 [D Loss: 0.649285, Acc.: 62.50%] [G Loss: 0.745524]\n",
      "640 [D Loss: 0.626542, Acc.: 59.38%] [G Loss: 0.753642]\n",
      "641 [D Loss: 0.668160, Acc.: 46.88%] [G Loss: 0.761898]\n",
      "642 [D Loss: 0.630940, Acc.: 62.50%] [G Loss: 0.788498]\n",
      "643 [D Loss: 0.674120, Acc.: 50.00%] [G Loss: 0.768108]\n",
      "644 [D Loss: 0.661809, Acc.: 56.25%] [G Loss: 0.761443]\n",
      "645 [D Loss: 0.670810, Acc.: 46.88%] [G Loss: 0.743382]\n",
      "646 [D Loss: 0.640030, Acc.: 59.38%] [G Loss: 0.718259]\n",
      "647 [D Loss: 0.649047, Acc.: 50.00%] [G Loss: 0.680112]\n",
      "648 [D Loss: 0.633126, Acc.: 56.25%] [G Loss: 0.682024]\n",
      "649 [D Loss: 0.633271, Acc.: 53.12%] [G Loss: 0.715795]\n",
      "650 [D Loss: 0.596046, Acc.: 59.38%] [G Loss: 0.745617]\n",
      "651 [D Loss: 0.633649, Acc.: 68.75%] [G Loss: 0.730626]\n",
      "652 [D Loss: 0.662296, Acc.: 50.00%] [G Loss: 0.765011]\n",
      "653 [D Loss: 0.621028, Acc.: 65.62%] [G Loss: 0.783338]\n",
      "654 [D Loss: 0.695464, Acc.: 50.00%] [G Loss: 0.744240]\n",
      "655 [D Loss: 0.626317, Acc.: 56.25%] [G Loss: 0.729225]\n",
      "656 [D Loss: 0.646327, Acc.: 65.62%] [G Loss: 0.725769]\n",
      "657 [D Loss: 0.681050, Acc.: 50.00%] [G Loss: 0.719055]\n",
      "658 [D Loss: 0.641273, Acc.: 50.00%] [G Loss: 0.725443]\n",
      "659 [D Loss: 0.642923, Acc.: 56.25%] [G Loss: 0.742513]\n",
      "660 [D Loss: 0.701164, Acc.: 37.50%] [G Loss: 0.722910]\n",
      "661 [D Loss: 0.638321, Acc.: 62.50%] [G Loss: 0.725204]\n",
      "662 [D Loss: 0.701279, Acc.: 34.38%] [G Loss: 0.707526]\n",
      "663 [D Loss: 0.650583, Acc.: 50.00%] [G Loss: 0.714595]\n",
      "664 [D Loss: 0.662306, Acc.: 50.00%] [G Loss: 0.726510]\n",
      "665 [D Loss: 0.638083, Acc.: 65.62%] [G Loss: 0.748895]\n",
      "666 [D Loss: 0.638425, Acc.: 65.62%] [G Loss: 0.699273]\n",
      "667 [D Loss: 0.679136, Acc.: 50.00%] [G Loss: 0.700620]\n",
      "668 [D Loss: 0.638469, Acc.: 56.25%] [G Loss: 0.714916]\n",
      "669 [D Loss: 0.621125, Acc.: 59.38%] [G Loss: 0.726419]\n",
      "670 [D Loss: 0.650271, Acc.: 53.12%] [G Loss: 0.718113]\n",
      "671 [D Loss: 0.662485, Acc.: 50.00%] [G Loss: 0.728073]\n",
      "672 [D Loss: 0.586491, Acc.: 71.88%] [G Loss: 0.708765]\n",
      "673 [D Loss: 0.628834, Acc.: 59.38%] [G Loss: 0.743692]\n",
      "674 [D Loss: 0.615063, Acc.: 62.50%] [G Loss: 0.748825]\n",
      "675 [D Loss: 0.634906, Acc.: 68.75%] [G Loss: 0.775280]\n",
      "676 [D Loss: 0.710366, Acc.: 46.88%] [G Loss: 0.743205]\n",
      "677 [D Loss: 0.671017, Acc.: 53.12%] [G Loss: 0.753849]\n",
      "678 [D Loss: 0.648125, Acc.: 59.38%] [G Loss: 0.770668]\n",
      "679 [D Loss: 0.632295, Acc.: 65.62%] [G Loss: 0.767227]\n",
      "680 [D Loss: 0.618389, Acc.: 65.62%] [G Loss: 0.747038]\n",
      "681 [D Loss: 0.664102, Acc.: 62.50%] [G Loss: 0.722373]\n",
      "682 [D Loss: 0.663510, Acc.: 50.00%] [G Loss: 0.734534]\n",
      "683 [D Loss: 0.635996, Acc.: 71.88%] [G Loss: 0.738933]\n",
      "684 [D Loss: 0.651761, Acc.: 53.12%] [G Loss: 0.745752]\n",
      "685 [D Loss: 0.656088, Acc.: 56.25%] [G Loss: 0.749197]\n",
      "686 [D Loss: 0.625492, Acc.: 65.62%] [G Loss: 0.758976]\n",
      "687 [D Loss: 0.631715, Acc.: 62.50%] [G Loss: 0.760198]\n",
      "688 [D Loss: 0.660495, Acc.: 62.50%] [G Loss: 0.739243]\n",
      "689 [D Loss: 0.649878, Acc.: 59.38%] [G Loss: 0.750028]\n",
      "690 [D Loss: 0.626056, Acc.: 75.00%] [G Loss: 0.764354]\n",
      "691 [D Loss: 0.681238, Acc.: 46.88%] [G Loss: 0.764471]\n",
      "692 [D Loss: 0.655108, Acc.: 53.12%] [G Loss: 0.777434]\n",
      "693 [D Loss: 0.659523, Acc.: 50.00%] [G Loss: 0.771370]\n",
      "694 [D Loss: 0.664173, Acc.: 46.88%] [G Loss: 0.773995]\n",
      "695 [D Loss: 0.642222, Acc.: 75.00%] [G Loss: 0.753852]\n",
      "696 [D Loss: 0.654704, Acc.: 53.12%] [G Loss: 0.730592]\n",
      "697 [D Loss: 0.648461, Acc.: 59.38%] [G Loss: 0.739761]\n",
      "698 [D Loss: 0.655082, Acc.: 46.88%] [G Loss: 0.770314]\n",
      "699 [D Loss: 0.647285, Acc.: 56.25%] [G Loss: 0.763972]\n",
      "700 [D Loss: 0.615347, Acc.: 68.75%] [G Loss: 0.786828]\n",
      "701 [D Loss: 0.622916, Acc.: 71.88%] [G Loss: 0.785035]\n",
      "702 [D Loss: 0.647502, Acc.: 59.38%] [G Loss: 0.760634]\n",
      "703 [D Loss: 0.656522, Acc.: 53.12%] [G Loss: 0.715810]\n",
      "704 [D Loss: 0.630763, Acc.: 59.38%] [G Loss: 0.734741]\n",
      "705 [D Loss: 0.640999, Acc.: 59.38%] [G Loss: 0.730619]\n",
      "706 [D Loss: 0.645022, Acc.: 59.38%] [G Loss: 0.717124]\n",
      "707 [D Loss: 0.641624, Acc.: 65.62%] [G Loss: 0.728534]\n",
      "708 [D Loss: 0.660263, Acc.: 53.12%] [G Loss: 0.776272]\n",
      "709 [D Loss: 0.639066, Acc.: 62.50%] [G Loss: 0.769361]\n",
      "710 [D Loss: 0.667872, Acc.: 56.25%] [G Loss: 0.737707]\n",
      "711 [D Loss: 0.647769, Acc.: 59.38%] [G Loss: 0.740795]\n",
      "712 [D Loss: 0.622520, Acc.: 65.62%] [G Loss: 0.729719]\n",
      "713 [D Loss: 0.628893, Acc.: 62.50%] [G Loss: 0.724402]\n",
      "714 [D Loss: 0.629794, Acc.: 53.12%] [G Loss: 0.725877]\n",
      "715 [D Loss: 0.640298, Acc.: 62.50%] [G Loss: 0.706094]\n",
      "716 [D Loss: 0.577836, Acc.: 71.88%] [G Loss: 0.714463]\n",
      "717 [D Loss: 0.637648, Acc.: 56.25%] [G Loss: 0.710673]\n",
      "718 [D Loss: 0.598545, Acc.: 65.62%] [G Loss: 0.759516]\n",
      "719 [D Loss: 0.600529, Acc.: 68.75%] [G Loss: 0.783157]\n",
      "720 [D Loss: 0.618754, Acc.: 68.75%] [G Loss: 0.788070]\n",
      "721 [D Loss: 0.662542, Acc.: 62.50%] [G Loss: 0.790198]\n",
      "722 [D Loss: 0.598403, Acc.: 75.00%] [G Loss: 0.772612]\n",
      "723 [D Loss: 0.583001, Acc.: 75.00%] [G Loss: 0.737004]\n",
      "724 [D Loss: 0.604597, Acc.: 65.62%] [G Loss: 0.720993]\n",
      "725 [D Loss: 0.598746, Acc.: 65.62%] [G Loss: 0.735154]\n",
      "726 [D Loss: 0.639081, Acc.: 50.00%] [G Loss: 0.772671]\n",
      "727 [D Loss: 0.620903, Acc.: 65.62%] [G Loss: 0.851494]\n",
      "728 [D Loss: 0.599339, Acc.: 75.00%] [G Loss: 0.851997]\n",
      "729 [D Loss: 0.641439, Acc.: 65.62%] [G Loss: 0.821102]\n",
      "730 [D Loss: 0.674263, Acc.: 46.88%] [G Loss: 0.772826]\n",
      "731 [D Loss: 0.643565, Acc.: 50.00%] [G Loss: 0.761234]\n",
      "732 [D Loss: 0.623196, Acc.: 62.50%] [G Loss: 0.735157]\n",
      "733 [D Loss: 0.578833, Acc.: 75.00%] [G Loss: 0.754357]\n",
      "734 [D Loss: 0.637536, Acc.: 59.38%] [G Loss: 0.781390]\n",
      "735 [D Loss: 0.632577, Acc.: 62.50%] [G Loss: 0.819500]\n",
      "736 [D Loss: 0.593733, Acc.: 75.00%] [G Loss: 0.815806]\n",
      "737 [D Loss: 0.576693, Acc.: 84.38%] [G Loss: 0.828936]\n",
      "738 [D Loss: 0.618771, Acc.: 62.50%] [G Loss: 0.790873]\n",
      "739 [D Loss: 0.609082, Acc.: 65.62%] [G Loss: 0.753736]\n",
      "740 [D Loss: 0.686575, Acc.: 50.00%] [G Loss: 0.715514]\n",
      "741 [D Loss: 0.668725, Acc.: 46.88%] [G Loss: 0.728467]\n",
      "742 [D Loss: 0.555078, Acc.: 81.25%] [G Loss: 0.782828]\n",
      "743 [D Loss: 0.636245, Acc.: 62.50%] [G Loss: 0.760141]\n",
      "744 [D Loss: 0.639690, Acc.: 62.50%] [G Loss: 0.745349]\n",
      "745 [D Loss: 0.554828, Acc.: 75.00%] [G Loss: 0.761867]\n",
      "746 [D Loss: 0.605764, Acc.: 68.75%] [G Loss: 0.781664]\n",
      "747 [D Loss: 0.603696, Acc.: 75.00%] [G Loss: 0.753916]\n",
      "748 [D Loss: 0.651449, Acc.: 62.50%] [G Loss: 0.765551]\n",
      "749 [D Loss: 0.640644, Acc.: 62.50%] [G Loss: 0.759272]\n",
      "750 [D Loss: 0.611910, Acc.: 62.50%] [G Loss: 0.791071]\n",
      "751 [D Loss: 0.616102, Acc.: 68.75%] [G Loss: 0.784724]\n",
      "752 [D Loss: 0.580024, Acc.: 68.75%] [G Loss: 0.819196]\n",
      "753 [D Loss: 0.622365, Acc.: 62.50%] [G Loss: 0.833145]\n",
      "754 [D Loss: 0.622725, Acc.: 71.88%] [G Loss: 0.814602]\n",
      "755 [D Loss: 0.600072, Acc.: 71.88%] [G Loss: 0.852584]\n",
      "756 [D Loss: 0.631606, Acc.: 68.75%] [G Loss: 0.818743]\n",
      "757 [D Loss: 0.652027, Acc.: 50.00%] [G Loss: 0.789664]\n",
      "758 [D Loss: 0.589314, Acc.: 68.75%] [G Loss: 0.772313]\n",
      "759 [D Loss: 0.593567, Acc.: 62.50%] [G Loss: 0.779332]\n",
      "760 [D Loss: 0.606899, Acc.: 75.00%] [G Loss: 0.768850]\n",
      "761 [D Loss: 0.660658, Acc.: 62.50%] [G Loss: 0.771409]\n",
      "762 [D Loss: 0.618678, Acc.: 71.88%] [G Loss: 0.791629]\n",
      "763 [D Loss: 0.573448, Acc.: 81.25%] [G Loss: 0.768965]\n",
      "764 [D Loss: 0.647090, Acc.: 65.62%] [G Loss: 0.757186]\n",
      "765 [D Loss: 0.619898, Acc.: 62.50%] [G Loss: 0.774004]\n",
      "766 [D Loss: 0.641007, Acc.: 59.38%] [G Loss: 0.763796]\n",
      "767 [D Loss: 0.589614, Acc.: 75.00%] [G Loss: 0.762779]\n",
      "768 [D Loss: 0.633947, Acc.: 56.25%] [G Loss: 0.791336]\n",
      "769 [D Loss: 0.585576, Acc.: 68.75%] [G Loss: 0.805002]\n",
      "770 [D Loss: 0.627805, Acc.: 62.50%] [G Loss: 0.822496]\n",
      "771 [D Loss: 0.599946, Acc.: 81.25%] [G Loss: 0.817368]\n",
      "772 [D Loss: 0.625943, Acc.: 68.75%] [G Loss: 0.820064]\n",
      "773 [D Loss: 0.624624, Acc.: 56.25%] [G Loss: 0.828329]\n",
      "774 [D Loss: 0.580132, Acc.: 75.00%] [G Loss: 0.833119]\n",
      "775 [D Loss: 0.637881, Acc.: 68.75%] [G Loss: 0.788913]\n",
      "776 [D Loss: 0.588919, Acc.: 71.88%] [G Loss: 0.802226]\n",
      "777 [D Loss: 0.628508, Acc.: 56.25%] [G Loss: 0.821615]\n",
      "778 [D Loss: 0.597797, Acc.: 71.88%] [G Loss: 0.891441]\n",
      "779 [D Loss: 0.570317, Acc.: 84.38%] [G Loss: 0.857468]\n",
      "780 [D Loss: 0.625773, Acc.: 75.00%] [G Loss: 0.814174]\n",
      "781 [D Loss: 0.629008, Acc.: 65.62%] [G Loss: 0.806538]\n",
      "782 [D Loss: 0.631396, Acc.: 59.38%] [G Loss: 0.792169]\n",
      "783 [D Loss: 0.563608, Acc.: 78.12%] [G Loss: 0.807463]\n",
      "784 [D Loss: 0.596331, Acc.: 68.75%] [G Loss: 0.819068]\n",
      "785 [D Loss: 0.613406, Acc.: 68.75%] [G Loss: 0.796092]\n",
      "786 [D Loss: 0.553992, Acc.: 71.88%] [G Loss: 0.837781]\n",
      "787 [D Loss: 0.614200, Acc.: 71.88%] [G Loss: 0.823862]\n",
      "788 [D Loss: 0.573799, Acc.: 78.12%] [G Loss: 0.836894]\n",
      "789 [D Loss: 0.655797, Acc.: 65.62%] [G Loss: 0.787051]\n",
      "790 [D Loss: 0.607099, Acc.: 62.50%] [G Loss: 0.788235]\n",
      "791 [D Loss: 0.626034, Acc.: 62.50%] [G Loss: 0.826713]\n",
      "792 [D Loss: 0.627312, Acc.: 75.00%] [G Loss: 0.792818]\n",
      "793 [D Loss: 0.624386, Acc.: 71.88%] [G Loss: 0.846268]\n",
      "794 [D Loss: 0.641472, Acc.: 59.38%] [G Loss: 0.836301]\n",
      "795 [D Loss: 0.599065, Acc.: 71.88%] [G Loss: 0.839557]\n",
      "796 [D Loss: 0.584906, Acc.: 75.00%] [G Loss: 0.816670]\n",
      "797 [D Loss: 0.608277, Acc.: 65.62%] [G Loss: 0.803447]\n",
      "798 [D Loss: 0.577747, Acc.: 78.12%] [G Loss: 0.814042]\n",
      "799 [D Loss: 0.612680, Acc.: 71.88%] [G Loss: 0.828713]\n",
      "800 [D Loss: 0.636323, Acc.: 75.00%] [G Loss: 0.779904]\n",
      "801 [D Loss: 0.617752, Acc.: 65.62%] [G Loss: 0.804586]\n",
      "802 [D Loss: 0.578935, Acc.: 71.88%] [G Loss: 0.817920]\n",
      "803 [D Loss: 0.558019, Acc.: 68.75%] [G Loss: 0.846628]\n",
      "804 [D Loss: 0.578667, Acc.: 71.88%] [G Loss: 0.826395]\n",
      "805 [D Loss: 0.600390, Acc.: 75.00%] [G Loss: 0.784079]\n",
      "806 [D Loss: 0.529952, Acc.: 71.88%] [G Loss: 0.806798]\n",
      "807 [D Loss: 0.596510, Acc.: 59.38%] [G Loss: 0.822096]\n",
      "808 [D Loss: 0.599104, Acc.: 71.88%] [G Loss: 0.800548]\n",
      "809 [D Loss: 0.592006, Acc.: 81.25%] [G Loss: 0.801343]\n",
      "810 [D Loss: 0.608135, Acc.: 62.50%] [G Loss: 0.810136]\n",
      "811 [D Loss: 0.647320, Acc.: 68.75%] [G Loss: 0.808647]\n",
      "812 [D Loss: 0.634245, Acc.: 56.25%] [G Loss: 0.778634]\n",
      "813 [D Loss: 0.545753, Acc.: 75.00%] [G Loss: 0.831815]\n",
      "814 [D Loss: 0.636978, Acc.: 50.00%] [G Loss: 0.804868]\n",
      "815 [D Loss: 0.575660, Acc.: 78.12%] [G Loss: 0.811449]\n",
      "816 [D Loss: 0.633285, Acc.: 75.00%] [G Loss: 0.829304]\n",
      "817 [D Loss: 0.627612, Acc.: 59.38%] [G Loss: 0.774172]\n",
      "818 [D Loss: 0.626194, Acc.: 59.38%] [G Loss: 0.825076]\n",
      "819 [D Loss: 0.619516, Acc.: 75.00%] [G Loss: 0.816808]\n",
      "820 [D Loss: 0.645984, Acc.: 65.62%] [G Loss: 0.810400]\n",
      "821 [D Loss: 0.635591, Acc.: 62.50%] [G Loss: 0.817608]\n",
      "822 [D Loss: 0.633864, Acc.: 75.00%] [G Loss: 0.763266]\n",
      "823 [D Loss: 0.630837, Acc.: 65.62%] [G Loss: 0.769452]\n",
      "824 [D Loss: 0.619113, Acc.: 62.50%] [G Loss: 0.775438]\n",
      "825 [D Loss: 0.698323, Acc.: 40.62%] [G Loss: 0.812456]\n",
      "826 [D Loss: 0.604952, Acc.: 71.88%] [G Loss: 0.810833]\n",
      "827 [D Loss: 0.601955, Acc.: 71.88%] [G Loss: 0.808608]\n",
      "828 [D Loss: 0.652474, Acc.: 53.12%] [G Loss: 0.739712]\n",
      "829 [D Loss: 0.618279, Acc.: 62.50%] [G Loss: 0.796939]\n",
      "830 [D Loss: 0.653403, Acc.: 62.50%] [G Loss: 0.769544]\n",
      "831 [D Loss: 0.647649, Acc.: 71.88%] [G Loss: 0.749544]\n",
      "832 [D Loss: 0.663521, Acc.: 65.62%] [G Loss: 0.744254]\n",
      "833 [D Loss: 0.580392, Acc.: 84.38%] [G Loss: 0.787610]\n",
      "834 [D Loss: 0.665350, Acc.: 46.88%] [G Loss: 0.783491]\n",
      "835 [D Loss: 0.651257, Acc.: 50.00%] [G Loss: 0.783592]\n",
      "836 [D Loss: 0.637004, Acc.: 65.62%] [G Loss: 0.789402]\n",
      "837 [D Loss: 0.623326, Acc.: 65.62%] [G Loss: 0.789144]\n",
      "838 [D Loss: 0.668606, Acc.: 40.62%] [G Loss: 0.767099]\n",
      "839 [D Loss: 0.632661, Acc.: 53.12%] [G Loss: 0.788242]\n",
      "840 [D Loss: 0.643247, Acc.: 65.62%] [G Loss: 0.777592]\n",
      "841 [D Loss: 0.620243, Acc.: 68.75%] [G Loss: 0.807489]\n",
      "842 [D Loss: 0.628389, Acc.: 62.50%] [G Loss: 0.771084]\n",
      "843 [D Loss: 0.648208, Acc.: 56.25%] [G Loss: 0.772850]\n",
      "844 [D Loss: 0.627346, Acc.: 68.75%] [G Loss: 0.783914]\n",
      "845 [D Loss: 0.657643, Acc.: 62.50%] [G Loss: 0.779114]\n",
      "846 [D Loss: 0.635518, Acc.: 62.50%] [G Loss: 0.825071]\n",
      "847 [D Loss: 0.612726, Acc.: 62.50%] [G Loss: 0.890575]\n",
      "848 [D Loss: 0.651298, Acc.: 62.50%] [G Loss: 0.823884]\n",
      "849 [D Loss: 0.672041, Acc.: 59.38%] [G Loss: 0.820164]\n",
      "850 [D Loss: 0.599027, Acc.: 68.75%] [G Loss: 0.811443]\n",
      "851 [D Loss: 0.638633, Acc.: 56.25%] [G Loss: 0.820538]\n",
      "852 [D Loss: 0.607796, Acc.: 71.88%] [G Loss: 0.797327]\n",
      "853 [D Loss: 0.631240, Acc.: 59.38%] [G Loss: 0.798863]\n",
      "854 [D Loss: 0.637483, Acc.: 62.50%] [G Loss: 0.820595]\n",
      "855 [D Loss: 0.642718, Acc.: 65.62%] [G Loss: 0.784132]\n",
      "856 [D Loss: 0.621717, Acc.: 65.62%] [G Loss: 0.761686]\n",
      "857 [D Loss: 0.629248, Acc.: 56.25%] [G Loss: 0.802371]\n",
      "858 [D Loss: 0.573739, Acc.: 78.12%] [G Loss: 0.803508]\n",
      "859 [D Loss: 0.628608, Acc.: 65.62%] [G Loss: 0.769334]\n",
      "860 [D Loss: 0.614758, Acc.: 68.75%] [G Loss: 0.832153]\n",
      "861 [D Loss: 0.588272, Acc.: 78.12%] [G Loss: 0.754836]\n",
      "862 [D Loss: 0.650586, Acc.: 53.12%] [G Loss: 0.734484]\n",
      "863 [D Loss: 0.643431, Acc.: 62.50%] [G Loss: 0.795211]\n",
      "864 [D Loss: 0.624840, Acc.: 62.50%] [G Loss: 0.832876]\n",
      "865 [D Loss: 0.691747, Acc.: 46.88%] [G Loss: 0.866131]\n",
      "866 [D Loss: 0.656699, Acc.: 65.62%] [G Loss: 0.798110]\n",
      "867 [D Loss: 0.627794, Acc.: 65.62%] [G Loss: 0.818746]\n",
      "868 [D Loss: 0.636954, Acc.: 59.38%] [G Loss: 0.822534]\n",
      "869 [D Loss: 0.615535, Acc.: 68.75%] [G Loss: 0.810279]\n",
      "870 [D Loss: 0.628552, Acc.: 68.75%] [G Loss: 0.789628]\n",
      "871 [D Loss: 0.625328, Acc.: 56.25%] [G Loss: 0.793938]\n",
      "872 [D Loss: 0.625587, Acc.: 71.88%] [G Loss: 0.776361]\n",
      "873 [D Loss: 0.680653, Acc.: 65.62%] [G Loss: 0.791424]\n",
      "874 [D Loss: 0.679514, Acc.: 59.38%] [G Loss: 0.799276]\n",
      "875 [D Loss: 0.598527, Acc.: 78.12%] [G Loss: 0.797411]\n",
      "876 [D Loss: 0.644099, Acc.: 50.00%] [G Loss: 0.762895]\n",
      "877 [D Loss: 0.627393, Acc.: 59.38%] [G Loss: 0.731942]\n",
      "878 [D Loss: 0.588540, Acc.: 71.88%] [G Loss: 0.704353]\n",
      "879 [D Loss: 0.597216, Acc.: 75.00%] [G Loss: 0.748299]\n",
      "880 [D Loss: 0.608240, Acc.: 81.25%] [G Loss: 0.751545]\n",
      "881 [D Loss: 0.608587, Acc.: 71.88%] [G Loss: 0.786402]\n",
      "882 [D Loss: 0.616144, Acc.: 71.88%] [G Loss: 0.802389]\n",
      "883 [D Loss: 0.620711, Acc.: 71.88%] [G Loss: 0.781514]\n",
      "884 [D Loss: 0.575475, Acc.: 71.88%] [G Loss: 0.785437]\n",
      "885 [D Loss: 0.614090, Acc.: 65.62%] [G Loss: 0.825858]\n",
      "886 [D Loss: 0.644581, Acc.: 62.50%] [G Loss: 0.822178]\n",
      "887 [D Loss: 0.601403, Acc.: 68.75%] [G Loss: 0.825252]\n",
      "888 [D Loss: 0.618203, Acc.: 65.62%] [G Loss: 0.828269]\n",
      "889 [D Loss: 0.607637, Acc.: 68.75%] [G Loss: 0.801666]\n",
      "890 [D Loss: 0.621903, Acc.: 68.75%] [G Loss: 0.783194]\n",
      "891 [D Loss: 0.656535, Acc.: 65.62%] [G Loss: 0.774618]\n",
      "892 [D Loss: 0.655110, Acc.: 53.12%] [G Loss: 0.763048]\n",
      "893 [D Loss: 0.635013, Acc.: 62.50%] [G Loss: 0.774781]\n",
      "894 [D Loss: 0.612078, Acc.: 68.75%] [G Loss: 0.767555]\n",
      "895 [D Loss: 0.647042, Acc.: 56.25%] [G Loss: 0.788424]\n",
      "896 [D Loss: 0.650103, Acc.: 62.50%] [G Loss: 0.811754]\n",
      "897 [D Loss: 0.610477, Acc.: 68.75%] [G Loss: 0.858882]\n",
      "898 [D Loss: 0.657711, Acc.: 65.62%] [G Loss: 0.803085]\n",
      "899 [D Loss: 0.614809, Acc.: 68.75%] [G Loss: 0.794662]\n",
      "900 [D Loss: 0.596063, Acc.: 78.12%] [G Loss: 0.788057]\n",
      "901 [D Loss: 0.642133, Acc.: 56.25%] [G Loss: 0.807916]\n",
      "902 [D Loss: 0.684029, Acc.: 53.12%] [G Loss: 0.818345]\n",
      "903 [D Loss: 0.598091, Acc.: 81.25%] [G Loss: 0.865334]\n",
      "904 [D Loss: 0.652462, Acc.: 65.62%] [G Loss: 0.812312]\n",
      "905 [D Loss: 0.596421, Acc.: 75.00%] [G Loss: 0.818913]\n",
      "906 [D Loss: 0.661945, Acc.: 59.38%] [G Loss: 0.797008]\n",
      "907 [D Loss: 0.643572, Acc.: 71.88%] [G Loss: 0.822127]\n",
      "908 [D Loss: 0.616607, Acc.: 81.25%] [G Loss: 0.790070]\n",
      "909 [D Loss: 0.622381, Acc.: 71.88%] [G Loss: 0.825179]\n",
      "910 [D Loss: 0.624489, Acc.: 65.62%] [G Loss: 0.780810]\n",
      "911 [D Loss: 0.624001, Acc.: 65.62%] [G Loss: 0.770073]\n",
      "912 [D Loss: 0.631295, Acc.: 62.50%] [G Loss: 0.777029]\n",
      "913 [D Loss: 0.678548, Acc.: 56.25%] [G Loss: 0.788835]\n",
      "914 [D Loss: 0.623549, Acc.: 50.00%] [G Loss: 0.781073]\n",
      "915 [D Loss: 0.621811, Acc.: 78.12%] [G Loss: 0.804786]\n",
      "916 [D Loss: 0.645962, Acc.: 65.62%] [G Loss: 0.809971]\n",
      "917 [D Loss: 0.631356, Acc.: 78.12%] [G Loss: 0.795215]\n",
      "918 [D Loss: 0.690333, Acc.: 50.00%] [G Loss: 0.747216]\n",
      "919 [D Loss: 0.607540, Acc.: 59.38%] [G Loss: 0.740208]\n",
      "920 [D Loss: 0.656735, Acc.: 50.00%] [G Loss: 0.750517]\n",
      "921 [D Loss: 0.671622, Acc.: 56.25%] [G Loss: 0.763829]\n",
      "922 [D Loss: 0.648854, Acc.: 71.88%] [G Loss: 0.798124]\n",
      "923 [D Loss: 0.620668, Acc.: 71.88%] [G Loss: 0.747979]\n",
      "924 [D Loss: 0.620320, Acc.: 59.38%] [G Loss: 0.742690]\n",
      "925 [D Loss: 0.666165, Acc.: 65.62%] [G Loss: 0.749150]\n",
      "926 [D Loss: 0.667971, Acc.: 56.25%] [G Loss: 0.736208]\n",
      "927 [D Loss: 0.654944, Acc.: 65.62%] [G Loss: 0.807221]\n",
      "928 [D Loss: 0.649139, Acc.: 62.50%] [G Loss: 0.778187]\n",
      "929 [D Loss: 0.614403, Acc.: 68.75%] [G Loss: 0.799255]\n",
      "930 [D Loss: 0.686193, Acc.: 50.00%] [G Loss: 0.818022]\n",
      "931 [D Loss: 0.683233, Acc.: 59.38%] [G Loss: 0.792292]\n",
      "932 [D Loss: 0.620135, Acc.: 68.75%] [G Loss: 0.816653]\n",
      "933 [D Loss: 0.632797, Acc.: 62.50%] [G Loss: 0.795050]\n",
      "934 [D Loss: 0.661125, Acc.: 68.75%] [G Loss: 0.749080]\n",
      "935 [D Loss: 0.646033, Acc.: 62.50%] [G Loss: 0.746795]\n",
      "936 [D Loss: 0.642681, Acc.: 46.88%] [G Loss: 0.756479]\n",
      "937 [D Loss: 0.653822, Acc.: 50.00%] [G Loss: 0.794943]\n",
      "938 [D Loss: 0.641459, Acc.: 62.50%] [G Loss: 0.782925]\n",
      "939 [D Loss: 0.635928, Acc.: 62.50%] [G Loss: 0.764423]\n",
      "940 [D Loss: 0.655269, Acc.: 50.00%] [G Loss: 0.790281]\n",
      "941 [D Loss: 0.639885, Acc.: 62.50%] [G Loss: 0.799462]\n",
      "942 [D Loss: 0.589032, Acc.: 81.25%] [G Loss: 0.814457]\n",
      "943 [D Loss: 0.674520, Acc.: 53.12%] [G Loss: 0.821366]\n",
      "944 [D Loss: 0.658643, Acc.: 65.62%] [G Loss: 0.829142]\n",
      "945 [D Loss: 0.661404, Acc.: 56.25%] [G Loss: 0.800411]\n",
      "946 [D Loss: 0.631709, Acc.: 56.25%] [G Loss: 0.802898]\n",
      "947 [D Loss: 0.600339, Acc.: 65.62%] [G Loss: 0.798239]\n",
      "948 [D Loss: 0.645716, Acc.: 71.88%] [G Loss: 0.839593]\n",
      "949 [D Loss: 0.600743, Acc.: 65.62%] [G Loss: 0.858618]\n",
      "950 [D Loss: 0.711244, Acc.: 46.88%] [G Loss: 0.859236]\n",
      "951 [D Loss: 0.620007, Acc.: 71.88%] [G Loss: 0.817348]\n",
      "952 [D Loss: 0.627922, Acc.: 65.62%] [G Loss: 0.835788]\n",
      "953 [D Loss: 0.626907, Acc.: 62.50%] [G Loss: 0.818932]\n",
      "954 [D Loss: 0.592949, Acc.: 81.25%] [G Loss: 0.775864]\n",
      "955 [D Loss: 0.601819, Acc.: 71.88%] [G Loss: 0.774339]\n",
      "956 [D Loss: 0.620069, Acc.: 65.62%] [G Loss: 0.732923]\n",
      "957 [D Loss: 0.669456, Acc.: 56.25%] [G Loss: 0.769590]\n",
      "958 [D Loss: 0.624615, Acc.: 59.38%] [G Loss: 0.848580]\n",
      "959 [D Loss: 0.647879, Acc.: 65.62%] [G Loss: 0.797172]\n",
      "960 [D Loss: 0.635432, Acc.: 62.50%] [G Loss: 0.823415]\n",
      "961 [D Loss: 0.631830, Acc.: 62.50%] [G Loss: 0.801235]\n",
      "962 [D Loss: 0.626381, Acc.: 71.88%] [G Loss: 0.825014]\n",
      "963 [D Loss: 0.631576, Acc.: 68.75%] [G Loss: 0.801495]\n",
      "964 [D Loss: 0.637920, Acc.: 62.50%] [G Loss: 0.789936]\n",
      "965 [D Loss: 0.642558, Acc.: 59.38%] [G Loss: 0.799043]\n",
      "966 [D Loss: 0.619809, Acc.: 65.62%] [G Loss: 0.771064]\n",
      "967 [D Loss: 0.602848, Acc.: 65.62%] [G Loss: 0.788267]\n",
      "968 [D Loss: 0.602409, Acc.: 68.75%] [G Loss: 0.779098]\n",
      "969 [D Loss: 0.642283, Acc.: 62.50%] [G Loss: 0.841682]\n",
      "970 [D Loss: 0.584355, Acc.: 75.00%] [G Loss: 0.816515]\n",
      "971 [D Loss: 0.600681, Acc.: 68.75%] [G Loss: 0.856919]\n",
      "972 [D Loss: 0.599750, Acc.: 78.12%] [G Loss: 0.817235]\n",
      "973 [D Loss: 0.656516, Acc.: 62.50%] [G Loss: 0.813216]\n",
      "974 [D Loss: 0.607614, Acc.: 68.75%] [G Loss: 0.769659]\n",
      "975 [D Loss: 0.653371, Acc.: 71.88%] [G Loss: 0.789249]\n",
      "976 [D Loss: 0.580707, Acc.: 78.12%] [G Loss: 0.865475]\n",
      "977 [D Loss: 0.633495, Acc.: 65.62%] [G Loss: 0.812161]\n",
      "978 [D Loss: 0.634853, Acc.: 65.62%] [G Loss: 0.835582]\n",
      "979 [D Loss: 0.654492, Acc.: 53.12%] [G Loss: 0.800815]\n",
      "980 [D Loss: 0.630698, Acc.: 65.62%] [G Loss: 0.796791]\n",
      "981 [D Loss: 0.591375, Acc.: 75.00%] [G Loss: 0.811173]\n",
      "982 [D Loss: 0.635636, Acc.: 56.25%] [G Loss: 0.791245]\n",
      "983 [D Loss: 0.598248, Acc.: 75.00%] [G Loss: 0.834122]\n",
      "984 [D Loss: 0.635357, Acc.: 68.75%] [G Loss: 0.774799]\n",
      "985 [D Loss: 0.596792, Acc.: 78.12%] [G Loss: 0.767671]\n",
      "986 [D Loss: 0.613130, Acc.: 65.62%] [G Loss: 0.762338]\n",
      "987 [D Loss: 0.706590, Acc.: 46.88%] [G Loss: 0.744146]\n",
      "988 [D Loss: 0.585101, Acc.: 68.75%] [G Loss: 0.779116]\n",
      "989 [D Loss: 0.632099, Acc.: 68.75%] [G Loss: 0.795325]\n",
      "990 [D Loss: 0.571917, Acc.: 81.25%] [G Loss: 0.784803]\n",
      "991 [D Loss: 0.636603, Acc.: 68.75%] [G Loss: 0.801498]\n",
      "992 [D Loss: 0.614482, Acc.: 62.50%] [G Loss: 0.845099]\n",
      "993 [D Loss: 0.618753, Acc.: 68.75%] [G Loss: 0.831934]\n",
      "994 [D Loss: 0.617664, Acc.: 71.88%] [G Loss: 0.791497]\n",
      "995 [D Loss: 0.600651, Acc.: 68.75%] [G Loss: 0.823126]\n",
      "996 [D Loss: 0.582949, Acc.: 78.12%] [G Loss: 0.841779]\n",
      "997 [D Loss: 0.609125, Acc.: 68.75%] [G Loss: 0.864474]\n",
      "998 [D Loss: 0.624792, Acc.: 65.62%] [G Loss: 0.824726]\n",
      "999 [D Loss: 0.597924, Acc.: 78.12%] [G Loss: 0.775981]\n",
      "1000 [D Loss: 0.599438, Acc.: 59.38%] [G Loss: 0.818798]\n",
      "1001 [D Loss: 0.617381, Acc.: 68.75%] [G Loss: 0.790489]\n",
      "1002 [D Loss: 0.579050, Acc.: 78.12%] [G Loss: 0.814726]\n",
      "1003 [D Loss: 0.647086, Acc.: 56.25%] [G Loss: 0.824509]\n",
      "1004 [D Loss: 0.582980, Acc.: 81.25%] [G Loss: 0.856787]\n",
      "1005 [D Loss: 0.645739, Acc.: 65.62%] [G Loss: 0.811929]\n",
      "1006 [D Loss: 0.609814, Acc.: 71.88%] [G Loss: 0.818302]\n",
      "1007 [D Loss: 0.570911, Acc.: 71.88%] [G Loss: 0.855516]\n",
      "1008 [D Loss: 0.641390, Acc.: 62.50%] [G Loss: 0.823075]\n",
      "1009 [D Loss: 0.628513, Acc.: 71.88%] [G Loss: 0.822979]\n",
      "1010 [D Loss: 0.555088, Acc.: 78.12%] [G Loss: 0.856740]\n",
      "1011 [D Loss: 0.626542, Acc.: 62.50%] [G Loss: 0.819035]\n",
      "1012 [D Loss: 0.594370, Acc.: 68.75%] [G Loss: 0.816766]\n",
      "1013 [D Loss: 0.622714, Acc.: 62.50%] [G Loss: 0.805329]\n",
      "1014 [D Loss: 0.571773, Acc.: 75.00%] [G Loss: 0.779705]\n",
      "1015 [D Loss: 0.598270, Acc.: 68.75%] [G Loss: 0.816213]\n",
      "1016 [D Loss: 0.593672, Acc.: 78.12%] [G Loss: 0.852631]\n",
      "1017 [D Loss: 0.573263, Acc.: 84.38%] [G Loss: 0.839043]\n",
      "1018 [D Loss: 0.624107, Acc.: 68.75%] [G Loss: 0.820413]\n",
      "1019 [D Loss: 0.552147, Acc.: 81.25%] [G Loss: 0.835086]\n",
      "1020 [D Loss: 0.586414, Acc.: 71.88%] [G Loss: 0.845924]\n",
      "1021 [D Loss: 0.622956, Acc.: 62.50%] [G Loss: 0.817983]\n",
      "1022 [D Loss: 0.589628, Acc.: 68.75%] [G Loss: 0.853061]\n",
      "1023 [D Loss: 0.631362, Acc.: 65.62%] [G Loss: 0.770540]\n",
      "1024 [D Loss: 0.605932, Acc.: 68.75%] [G Loss: 0.772997]\n",
      "1025 [D Loss: 0.578457, Acc.: 75.00%] [G Loss: 0.797732]\n",
      "1026 [D Loss: 0.649764, Acc.: 59.38%] [G Loss: 0.782205]\n",
      "1027 [D Loss: 0.604943, Acc.: 62.50%] [G Loss: 0.824800]\n",
      "1028 [D Loss: 0.604626, Acc.: 68.75%] [G Loss: 0.818316]\n",
      "1029 [D Loss: 0.598234, Acc.: 75.00%] [G Loss: 0.810331]\n",
      "1030 [D Loss: 0.592668, Acc.: 81.25%] [G Loss: 0.821620]\n",
      "1031 [D Loss: 0.552771, Acc.: 78.12%] [G Loss: 0.833445]\n",
      "1032 [D Loss: 0.661087, Acc.: 65.62%] [G Loss: 0.813796]\n",
      "1033 [D Loss: 0.577962, Acc.: 78.12%] [G Loss: 0.819711]\n",
      "1034 [D Loss: 0.586183, Acc.: 75.00%] [G Loss: 0.773176]\n",
      "1035 [D Loss: 0.557901, Acc.: 75.00%] [G Loss: 0.808316]\n",
      "1036 [D Loss: 0.571357, Acc.: 81.25%] [G Loss: 0.882105]\n",
      "1037 [D Loss: 0.636841, Acc.: 59.38%] [G Loss: 0.810997]\n",
      "1038 [D Loss: 0.594029, Acc.: 65.62%] [G Loss: 0.773104]\n",
      "1039 [D Loss: 0.619272, Acc.: 65.62%] [G Loss: 0.782871]\n",
      "1040 [D Loss: 0.631529, Acc.: 65.62%] [G Loss: 0.800978]\n",
      "1041 [D Loss: 0.625252, Acc.: 59.38%] [G Loss: 0.801412]\n",
      "1042 [D Loss: 0.563742, Acc.: 68.75%] [G Loss: 0.809703]\n",
      "1043 [D Loss: 0.647816, Acc.: 43.75%] [G Loss: 0.799547]\n",
      "1044 [D Loss: 0.640490, Acc.: 62.50%] [G Loss: 0.792432]\n",
      "1045 [D Loss: 0.633700, Acc.: 59.38%] [G Loss: 0.846988]\n",
      "1046 [D Loss: 0.626562, Acc.: 71.88%] [G Loss: 0.872835]\n",
      "1047 [D Loss: 0.661633, Acc.: 56.25%] [G Loss: 0.814133]\n",
      "1048 [D Loss: 0.587176, Acc.: 71.88%] [G Loss: 0.821323]\n",
      "1049 [D Loss: 0.611685, Acc.: 78.12%] [G Loss: 0.828333]\n",
      "1050 [D Loss: 0.627997, Acc.: 71.88%] [G Loss: 0.815900]\n",
      "1051 [D Loss: 0.593805, Acc.: 71.88%] [G Loss: 0.832994]\n",
      "1052 [D Loss: 0.591693, Acc.: 68.75%] [G Loss: 0.841297]\n",
      "1053 [D Loss: 0.607393, Acc.: 68.75%] [G Loss: 0.879729]\n",
      "1054 [D Loss: 0.628478, Acc.: 81.25%] [G Loss: 0.785505]\n",
      "1055 [D Loss: 0.669913, Acc.: 59.38%] [G Loss: 0.758654]\n",
      "1056 [D Loss: 0.644136, Acc.: 65.62%] [G Loss: 0.761197]\n",
      "1057 [D Loss: 0.655489, Acc.: 53.12%] [G Loss: 0.820241]\n",
      "1058 [D Loss: 0.641906, Acc.: 62.50%] [G Loss: 0.777741]\n",
      "1059 [D Loss: 0.621622, Acc.: 62.50%] [G Loss: 0.804316]\n",
      "1060 [D Loss: 0.645596, Acc.: 65.62%] [G Loss: 0.802012]\n",
      "1061 [D Loss: 0.632602, Acc.: 62.50%] [G Loss: 0.777052]\n",
      "1062 [D Loss: 0.639527, Acc.: 62.50%] [G Loss: 0.803079]\n",
      "1063 [D Loss: 0.636374, Acc.: 65.62%] [G Loss: 0.814057]\n",
      "1064 [D Loss: 0.646090, Acc.: 53.12%] [G Loss: 0.802502]\n",
      "1065 [D Loss: 0.606648, Acc.: 68.75%] [G Loss: 0.845765]\n",
      "1066 [D Loss: 0.629096, Acc.: 71.88%] [G Loss: 0.809694]\n",
      "1067 [D Loss: 0.620894, Acc.: 62.50%] [G Loss: 0.773227]\n",
      "1068 [D Loss: 0.636164, Acc.: 62.50%] [G Loss: 0.804159]\n",
      "1069 [D Loss: 0.631021, Acc.: 65.62%] [G Loss: 0.794484]\n",
      "1070 [D Loss: 0.650692, Acc.: 53.12%] [G Loss: 0.785403]\n",
      "1071 [D Loss: 0.582206, Acc.: 81.25%] [G Loss: 0.820340]\n",
      "1072 [D Loss: 0.688560, Acc.: 53.12%] [G Loss: 0.803339]\n",
      "1073 [D Loss: 0.658514, Acc.: 65.62%] [G Loss: 0.795750]\n",
      "1074 [D Loss: 0.627109, Acc.: 68.75%] [G Loss: 0.748080]\n",
      "1075 [D Loss: 0.646647, Acc.: 65.62%] [G Loss: 0.806986]\n",
      "1076 [D Loss: 0.630939, Acc.: 59.38%] [G Loss: 0.845180]\n",
      "1077 [D Loss: 0.631645, Acc.: 62.50%] [G Loss: 0.824778]\n",
      "1078 [D Loss: 0.624109, Acc.: 65.62%] [G Loss: 0.816373]\n",
      "1079 [D Loss: 0.625110, Acc.: 71.88%] [G Loss: 0.815055]\n",
      "1080 [D Loss: 0.647762, Acc.: 53.12%] [G Loss: 0.820713]\n",
      "1081 [D Loss: 0.607209, Acc.: 68.75%] [G Loss: 0.843318]\n",
      "1082 [D Loss: 0.595992, Acc.: 75.00%] [G Loss: 0.870649]\n",
      "1083 [D Loss: 0.690314, Acc.: 56.25%] [G Loss: 0.786947]\n",
      "1084 [D Loss: 0.636475, Acc.: 65.62%] [G Loss: 0.757939]\n",
      "1085 [D Loss: 0.603639, Acc.: 65.62%] [G Loss: 0.785880]\n",
      "1086 [D Loss: 0.620719, Acc.: 59.38%] [G Loss: 0.835157]\n",
      "1087 [D Loss: 0.623916, Acc.: 68.75%] [G Loss: 0.811759]\n",
      "1088 [D Loss: 0.608263, Acc.: 78.12%] [G Loss: 0.804510]\n",
      "1089 [D Loss: 0.558141, Acc.: 75.00%] [G Loss: 0.789790]\n",
      "1090 [D Loss: 0.631414, Acc.: 71.88%] [G Loss: 0.830258]\n",
      "1091 [D Loss: 0.551736, Acc.: 87.50%] [G Loss: 0.895892]\n",
      "1092 [D Loss: 0.621340, Acc.: 65.62%] [G Loss: 0.855319]\n",
      "1093 [D Loss: 0.669778, Acc.: 50.00%] [G Loss: 0.814729]\n",
      "1094 [D Loss: 0.599088, Acc.: 81.25%] [G Loss: 0.798504]\n",
      "1095 [D Loss: 0.566550, Acc.: 75.00%] [G Loss: 0.838208]\n",
      "1096 [D Loss: 0.613749, Acc.: 59.38%] [G Loss: 0.867513]\n",
      "1097 [D Loss: 0.547251, Acc.: 75.00%] [G Loss: 0.891886]\n",
      "1098 [D Loss: 0.639195, Acc.: 62.50%] [G Loss: 0.865641]\n",
      "1099 [D Loss: 0.598203, Acc.: 71.88%] [G Loss: 0.863610]\n",
      "1100 [D Loss: 0.683303, Acc.: 50.00%] [G Loss: 0.843407]\n",
      "1101 [D Loss: 0.548697, Acc.: 81.25%] [G Loss: 0.901061]\n",
      "1102 [D Loss: 0.671272, Acc.: 43.75%] [G Loss: 0.837837]\n",
      "1103 [D Loss: 0.676706, Acc.: 46.88%] [G Loss: 0.783946]\n",
      "1104 [D Loss: 0.690948, Acc.: 50.00%] [G Loss: 0.789976]\n",
      "1105 [D Loss: 0.641750, Acc.: 53.12%] [G Loss: 0.808568]\n",
      "1106 [D Loss: 0.590560, Acc.: 87.50%] [G Loss: 0.823311]\n",
      "1107 [D Loss: 0.648824, Acc.: 68.75%] [G Loss: 0.820564]\n",
      "1108 [D Loss: 0.618766, Acc.: 71.88%] [G Loss: 0.842989]\n",
      "1109 [D Loss: 0.691423, Acc.: 56.25%] [G Loss: 0.757377]\n",
      "1110 [D Loss: 0.616688, Acc.: 65.62%] [G Loss: 0.757334]\n",
      "1111 [D Loss: 0.626584, Acc.: 65.62%] [G Loss: 0.782796]\n",
      "1112 [D Loss: 0.655290, Acc.: 59.38%] [G Loss: 0.823466]\n",
      "1113 [D Loss: 0.676500, Acc.: 62.50%] [G Loss: 0.863644]\n",
      "1114 [D Loss: 0.643224, Acc.: 71.88%] [G Loss: 0.802036]\n",
      "1115 [D Loss: 0.649275, Acc.: 65.62%] [G Loss: 0.840555]\n",
      "1116 [D Loss: 0.688224, Acc.: 56.25%] [G Loss: 0.787124]\n",
      "1117 [D Loss: 0.656344, Acc.: 62.50%] [G Loss: 0.846677]\n",
      "1118 [D Loss: 0.661062, Acc.: 65.62%] [G Loss: 0.837464]\n",
      "1119 [D Loss: 0.701540, Acc.: 50.00%] [G Loss: 0.817773]\n",
      "1120 [D Loss: 0.613931, Acc.: 65.62%] [G Loss: 0.842789]\n",
      "1121 [D Loss: 0.676502, Acc.: 56.25%] [G Loss: 0.823317]\n",
      "1122 [D Loss: 0.602968, Acc.: 78.12%] [G Loss: 0.815767]\n",
      "1123 [D Loss: 0.647514, Acc.: 68.75%] [G Loss: 0.744105]\n",
      "1124 [D Loss: 0.642132, Acc.: 62.50%] [G Loss: 0.809711]\n",
      "1125 [D Loss: 0.624853, Acc.: 65.62%] [G Loss: 0.863517]\n",
      "1126 [D Loss: 0.602696, Acc.: 75.00%] [G Loss: 0.830558]\n",
      "1127 [D Loss: 0.680006, Acc.: 56.25%] [G Loss: 0.788483]\n",
      "1128 [D Loss: 0.642976, Acc.: 65.62%] [G Loss: 0.836881]\n",
      "1129 [D Loss: 0.649276, Acc.: 68.75%] [G Loss: 0.792896]\n",
      "1130 [D Loss: 0.659209, Acc.: 62.50%] [G Loss: 0.802153]\n",
      "1131 [D Loss: 0.621821, Acc.: 75.00%] [G Loss: 0.769130]\n",
      "1132 [D Loss: 0.648427, Acc.: 53.12%] [G Loss: 0.816136]\n",
      "1133 [D Loss: 0.603394, Acc.: 84.38%] [G Loss: 0.827508]\n",
      "1134 [D Loss: 0.587745, Acc.: 81.25%] [G Loss: 0.823108]\n",
      "1135 [D Loss: 0.613791, Acc.: 62.50%] [G Loss: 0.812442]\n",
      "1136 [D Loss: 0.625303, Acc.: 75.00%] [G Loss: 0.770099]\n",
      "1137 [D Loss: 0.643685, Acc.: 65.62%] [G Loss: 0.798021]\n",
      "1138 [D Loss: 0.591368, Acc.: 81.25%] [G Loss: 0.800871]\n",
      "1139 [D Loss: 0.630639, Acc.: 71.88%] [G Loss: 0.853256]\n",
      "1140 [D Loss: 0.622100, Acc.: 62.50%] [G Loss: 0.811730]\n",
      "1141 [D Loss: 0.625399, Acc.: 62.50%] [G Loss: 0.847808]\n",
      "1142 [D Loss: 0.656627, Acc.: 59.38%] [G Loss: 0.829640]\n",
      "1143 [D Loss: 0.659642, Acc.: 46.88%] [G Loss: 0.831916]\n",
      "1144 [D Loss: 0.636215, Acc.: 65.62%] [G Loss: 0.826627]\n",
      "1145 [D Loss: 0.579868, Acc.: 81.25%] [G Loss: 0.817314]\n",
      "1146 [D Loss: 0.609332, Acc.: 71.88%] [G Loss: 0.761033]\n",
      "1147 [D Loss: 0.620721, Acc.: 71.88%] [G Loss: 0.800628]\n",
      "1148 [D Loss: 0.619612, Acc.: 75.00%] [G Loss: 0.791169]\n",
      "1149 [D Loss: 0.587971, Acc.: 78.12%] [G Loss: 0.780033]\n",
      "1150 [D Loss: 0.611781, Acc.: 68.75%] [G Loss: 0.818457]\n",
      "1151 [D Loss: 0.640248, Acc.: 62.50%] [G Loss: 0.791988]\n",
      "1152 [D Loss: 0.655651, Acc.: 62.50%] [G Loss: 0.786870]\n",
      "1153 [D Loss: 0.642328, Acc.: 65.62%] [G Loss: 0.769855]\n",
      "1154 [D Loss: 0.607451, Acc.: 75.00%] [G Loss: 0.809893]\n",
      "1155 [D Loss: 0.632082, Acc.: 71.88%] [G Loss: 0.815539]\n",
      "1156 [D Loss: 0.597509, Acc.: 81.25%] [G Loss: 0.782656]\n",
      "1157 [D Loss: 0.654520, Acc.: 68.75%] [G Loss: 0.836145]\n",
      "1158 [D Loss: 0.645166, Acc.: 59.38%] [G Loss: 0.835507]\n",
      "1159 [D Loss: 0.621327, Acc.: 71.88%] [G Loss: 0.848864]\n",
      "1160 [D Loss: 0.636106, Acc.: 68.75%] [G Loss: 0.822103]\n",
      "1161 [D Loss: 0.665751, Acc.: 68.75%] [G Loss: 0.805737]\n",
      "1162 [D Loss: 0.622382, Acc.: 68.75%] [G Loss: 0.836435]\n",
      "1163 [D Loss: 0.635177, Acc.: 65.62%] [G Loss: 0.843024]\n",
      "1164 [D Loss: 0.678873, Acc.: 53.12%] [G Loss: 0.815733]\n",
      "1165 [D Loss: 0.586196, Acc.: 75.00%] [G Loss: 0.845497]\n",
      "1166 [D Loss: 0.611815, Acc.: 68.75%] [G Loss: 0.855485]\n",
      "1167 [D Loss: 0.615494, Acc.: 75.00%] [G Loss: 0.842193]\n",
      "1168 [D Loss: 0.621022, Acc.: 71.88%] [G Loss: 0.817428]\n",
      "1169 [D Loss: 0.628695, Acc.: 68.75%] [G Loss: 0.816976]\n",
      "1170 [D Loss: 0.604829, Acc.: 84.38%] [G Loss: 0.825602]\n",
      "1171 [D Loss: 0.616248, Acc.: 68.75%] [G Loss: 0.792086]\n",
      "1172 [D Loss: 0.645957, Acc.: 62.50%] [G Loss: 0.796346]\n",
      "1173 [D Loss: 0.631659, Acc.: 56.25%] [G Loss: 0.817382]\n",
      "1174 [D Loss: 0.617846, Acc.: 65.62%] [G Loss: 0.831242]\n",
      "1175 [D Loss: 0.599404, Acc.: 68.75%] [G Loss: 0.824480]\n",
      "1176 [D Loss: 0.596589, Acc.: 81.25%] [G Loss: 0.810117]\n",
      "1177 [D Loss: 0.632847, Acc.: 68.75%] [G Loss: 0.842082]\n",
      "1178 [D Loss: 0.604164, Acc.: 71.88%] [G Loss: 0.841540]\n",
      "1179 [D Loss: 0.607043, Acc.: 71.88%] [G Loss: 0.846648]\n",
      "1180 [D Loss: 0.600104, Acc.: 78.12%] [G Loss: 0.831123]\n",
      "1181 [D Loss: 0.582187, Acc.: 78.12%] [G Loss: 0.846531]\n",
      "1182 [D Loss: 0.587454, Acc.: 75.00%] [G Loss: 0.914046]\n",
      "1183 [D Loss: 0.615411, Acc.: 75.00%] [G Loss: 0.842946]\n",
      "1184 [D Loss: 0.570161, Acc.: 68.75%] [G Loss: 0.838764]\n",
      "1185 [D Loss: 0.630536, Acc.: 68.75%] [G Loss: 0.845866]\n",
      "1186 [D Loss: 0.612465, Acc.: 65.62%] [G Loss: 0.837134]\n",
      "1187 [D Loss: 0.558174, Acc.: 71.88%] [G Loss: 0.791729]\n",
      "1188 [D Loss: 0.613662, Acc.: 62.50%] [G Loss: 0.817622]\n",
      "1189 [D Loss: 0.573278, Acc.: 75.00%] [G Loss: 0.818309]\n",
      "1190 [D Loss: 0.646450, Acc.: 62.50%] [G Loss: 0.801538]\n",
      "1191 [D Loss: 0.663482, Acc.: 50.00%] [G Loss: 0.789461]\n",
      "1192 [D Loss: 0.577781, Acc.: 71.88%] [G Loss: 0.785798]\n",
      "1193 [D Loss: 0.641185, Acc.: 62.50%] [G Loss: 0.752494]\n",
      "1194 [D Loss: 0.634757, Acc.: 62.50%] [G Loss: 0.806574]\n",
      "1195 [D Loss: 0.648997, Acc.: 56.25%] [G Loss: 0.826603]\n",
      "1196 [D Loss: 0.644839, Acc.: 56.25%] [G Loss: 0.790789]\n",
      "1197 [D Loss: 0.626836, Acc.: 65.62%] [G Loss: 0.776878]\n",
      "1198 [D Loss: 0.630395, Acc.: 68.75%] [G Loss: 0.755663]\n",
      "1199 [D Loss: 0.621849, Acc.: 65.62%] [G Loss: 0.793542]\n",
      "1200 [D Loss: 0.594801, Acc.: 78.12%] [G Loss: 0.790514]\n",
      "1201 [D Loss: 0.753616, Acc.: 43.75%] [G Loss: 0.827071]\n",
      "1202 [D Loss: 0.640953, Acc.: 59.38%] [G Loss: 0.852333]\n",
      "1203 [D Loss: 0.610677, Acc.: 75.00%] [G Loss: 0.815147]\n",
      "1204 [D Loss: 0.594847, Acc.: 68.75%] [G Loss: 0.823924]\n",
      "1205 [D Loss: 0.645122, Acc.: 65.62%] [G Loss: 0.847697]\n",
      "1206 [D Loss: 0.664759, Acc.: 75.00%] [G Loss: 0.887973]\n",
      "1207 [D Loss: 0.648945, Acc.: 65.62%] [G Loss: 0.808192]\n",
      "1208 [D Loss: 0.644575, Acc.: 62.50%] [G Loss: 0.797726]\n",
      "1209 [D Loss: 0.660223, Acc.: 65.62%] [G Loss: 0.778652]\n",
      "1210 [D Loss: 0.606405, Acc.: 65.62%] [G Loss: 0.824932]\n",
      "1211 [D Loss: 0.665363, Acc.: 56.25%] [G Loss: 0.797662]\n",
      "1212 [D Loss: 0.560574, Acc.: 75.00%] [G Loss: 0.831586]\n",
      "1213 [D Loss: 0.617306, Acc.: 75.00%] [G Loss: 0.846334]\n",
      "1214 [D Loss: 0.614864, Acc.: 71.88%] [G Loss: 0.863773]\n",
      "1215 [D Loss: 0.623417, Acc.: 62.50%] [G Loss: 0.832803]\n",
      "1216 [D Loss: 0.599648, Acc.: 78.12%] [G Loss: 0.871167]\n",
      "1217 [D Loss: 0.683627, Acc.: 56.25%] [G Loss: 0.786148]\n",
      "1218 [D Loss: 0.627752, Acc.: 65.62%] [G Loss: 0.804374]\n",
      "1219 [D Loss: 0.635000, Acc.: 65.62%] [G Loss: 0.812150]\n",
      "1220 [D Loss: 0.624085, Acc.: 62.50%] [G Loss: 0.811661]\n",
      "1221 [D Loss: 0.687668, Acc.: 50.00%] [G Loss: 0.804997]\n",
      "1222 [D Loss: 0.629359, Acc.: 59.38%] [G Loss: 0.816479]\n",
      "1223 [D Loss: 0.671064, Acc.: 59.38%] [G Loss: 0.827433]\n",
      "1224 [D Loss: 0.589180, Acc.: 71.88%] [G Loss: 0.846297]\n",
      "1225 [D Loss: 0.603011, Acc.: 75.00%] [G Loss: 0.864441]\n",
      "1226 [D Loss: 0.639645, Acc.: 65.62%] [G Loss: 0.816725]\n",
      "1227 [D Loss: 0.645308, Acc.: 62.50%] [G Loss: 0.794750]\n",
      "1228 [D Loss: 0.578682, Acc.: 81.25%] [G Loss: 0.758670]\n",
      "1229 [D Loss: 0.642680, Acc.: 68.75%] [G Loss: 0.745292]\n",
      "1230 [D Loss: 0.631884, Acc.: 62.50%] [G Loss: 0.814577]\n",
      "1231 [D Loss: 0.601517, Acc.: 62.50%] [G Loss: 0.807244]\n",
      "1232 [D Loss: 0.672318, Acc.: 53.12%] [G Loss: 0.815670]\n",
      "1233 [D Loss: 0.640936, Acc.: 59.38%] [G Loss: 0.815014]\n",
      "1234 [D Loss: 0.583429, Acc.: 81.25%] [G Loss: 0.813567]\n",
      "1235 [D Loss: 0.572148, Acc.: 78.12%] [G Loss: 0.894011]\n",
      "1236 [D Loss: 0.619439, Acc.: 75.00%] [G Loss: 0.857921]\n",
      "1237 [D Loss: 0.626955, Acc.: 68.75%] [G Loss: 0.781368]\n",
      "1238 [D Loss: 0.638284, Acc.: 59.38%] [G Loss: 0.817473]\n",
      "1239 [D Loss: 0.640138, Acc.: 53.12%] [G Loss: 0.826558]\n",
      "1240 [D Loss: 0.614165, Acc.: 65.62%] [G Loss: 0.885981]\n",
      "1241 [D Loss: 0.623131, Acc.: 65.62%] [G Loss: 0.843733]\n",
      "1242 [D Loss: 0.653619, Acc.: 62.50%] [G Loss: 0.831998]\n",
      "1243 [D Loss: 0.598561, Acc.: 71.88%] [G Loss: 0.817934]\n",
      "1244 [D Loss: 0.594098, Acc.: 84.38%] [G Loss: 0.833342]\n",
      "1245 [D Loss: 0.644842, Acc.: 56.25%] [G Loss: 0.832226]\n",
      "1246 [D Loss: 0.583281, Acc.: 75.00%] [G Loss: 0.859893]\n",
      "1247 [D Loss: 0.572506, Acc.: 75.00%] [G Loss: 0.892932]\n",
      "1248 [D Loss: 0.668628, Acc.: 62.50%] [G Loss: 0.763647]\n",
      "1249 [D Loss: 0.570223, Acc.: 81.25%] [G Loss: 0.835021]\n",
      "1250 [D Loss: 0.619932, Acc.: 75.00%] [G Loss: 0.783467]\n",
      "1251 [D Loss: 0.591362, Acc.: 65.62%] [G Loss: 0.860814]\n",
      "1252 [D Loss: 0.626056, Acc.: 56.25%] [G Loss: 0.847258]\n",
      "1253 [D Loss: 0.621790, Acc.: 71.88%] [G Loss: 0.817757]\n",
      "1254 [D Loss: 0.582411, Acc.: 75.00%] [G Loss: 0.842275]\n",
      "1255 [D Loss: 0.532733, Acc.: 78.12%] [G Loss: 0.838828]\n",
      "1256 [D Loss: 0.637889, Acc.: 56.25%] [G Loss: 0.822648]\n",
      "1257 [D Loss: 0.603863, Acc.: 71.88%] [G Loss: 0.841326]\n",
      "1258 [D Loss: 0.630064, Acc.: 62.50%] [G Loss: 0.845778]\n",
      "1259 [D Loss: 0.615350, Acc.: 71.88%] [G Loss: 0.832231]\n",
      "1260 [D Loss: 0.603878, Acc.: 75.00%] [G Loss: 0.840017]\n",
      "1261 [D Loss: 0.573445, Acc.: 71.88%] [G Loss: 0.849090]\n",
      "1262 [D Loss: 0.596001, Acc.: 84.38%] [G Loss: 0.845528]\n",
      "1263 [D Loss: 0.631637, Acc.: 68.75%] [G Loss: 0.820753]\n",
      "1264 [D Loss: 0.556377, Acc.: 78.12%] [G Loss: 0.820423]\n",
      "1265 [D Loss: 0.613278, Acc.: 71.88%] [G Loss: 0.845547]\n",
      "1266 [D Loss: 0.561945, Acc.: 78.12%] [G Loss: 0.863705]\n",
      "1267 [D Loss: 0.619284, Acc.: 75.00%] [G Loss: 0.851739]\n",
      "1268 [D Loss: 0.605665, Acc.: 68.75%] [G Loss: 0.895580]\n",
      "1269 [D Loss: 0.569397, Acc.: 75.00%] [G Loss: 0.877280]\n",
      "1270 [D Loss: 0.569400, Acc.: 75.00%] [G Loss: 0.869310]\n",
      "1271 [D Loss: 0.633168, Acc.: 65.62%] [G Loss: 0.867074]\n",
      "1272 [D Loss: 0.646847, Acc.: 62.50%] [G Loss: 0.918335]\n",
      "1273 [D Loss: 0.601805, Acc.: 65.62%] [G Loss: 0.899837]\n",
      "1274 [D Loss: 0.549911, Acc.: 84.38%] [G Loss: 0.917565]\n",
      "1275 [D Loss: 0.570950, Acc.: 84.38%] [G Loss: 0.916492]\n",
      "1276 [D Loss: 0.609988, Acc.: 75.00%] [G Loss: 0.894249]\n",
      "1277 [D Loss: 0.559053, Acc.: 81.25%] [G Loss: 0.902876]\n",
      "1278 [D Loss: 0.584701, Acc.: 71.88%] [G Loss: 0.867304]\n",
      "1279 [D Loss: 0.644965, Acc.: 53.12%] [G Loss: 0.895087]\n",
      "1280 [D Loss: 0.638862, Acc.: 65.62%] [G Loss: 0.910414]\n",
      "1281 [D Loss: 0.607257, Acc.: 78.12%] [G Loss: 0.820092]\n",
      "1282 [D Loss: 0.574137, Acc.: 75.00%] [G Loss: 0.884356]\n",
      "1283 [D Loss: 0.567119, Acc.: 87.50%] [G Loss: 0.833007]\n",
      "1284 [D Loss: 0.568921, Acc.: 75.00%] [G Loss: 0.771102]\n",
      "1285 [D Loss: 0.625914, Acc.: 62.50%] [G Loss: 0.767626]\n",
      "1286 [D Loss: 0.537228, Acc.: 75.00%] [G Loss: 0.814152]\n",
      "1287 [D Loss: 0.626634, Acc.: 56.25%] [G Loss: 0.806149]\n",
      "1288 [D Loss: 0.560205, Acc.: 71.88%] [G Loss: 0.872421]\n",
      "1289 [D Loss: 0.560902, Acc.: 81.25%] [G Loss: 0.913403]\n",
      "1290 [D Loss: 0.684153, Acc.: 50.00%] [G Loss: 0.906538]\n",
      "1291 [D Loss: 0.611785, Acc.: 78.12%] [G Loss: 0.855867]\n",
      "1292 [D Loss: 0.610132, Acc.: 65.62%] [G Loss: 0.783597]\n",
      "1293 [D Loss: 0.554192, Acc.: 78.12%] [G Loss: 0.808600]\n",
      "1294 [D Loss: 0.606184, Acc.: 68.75%] [G Loss: 0.800497]\n",
      "1295 [D Loss: 0.625424, Acc.: 59.38%] [G Loss: 0.852463]\n",
      "1296 [D Loss: 0.556438, Acc.: 71.88%] [G Loss: 0.917389]\n",
      "1297 [D Loss: 0.603964, Acc.: 75.00%] [G Loss: 0.856224]\n",
      "1298 [D Loss: 0.625596, Acc.: 65.62%] [G Loss: 0.848056]\n",
      "1299 [D Loss: 0.635786, Acc.: 62.50%] [G Loss: 0.829680]\n",
      "1300 [D Loss: 0.573150, Acc.: 78.12%] [G Loss: 0.816117]\n",
      "1301 [D Loss: 0.664575, Acc.: 65.62%] [G Loss: 0.853866]\n",
      "1302 [D Loss: 0.590308, Acc.: 75.00%] [G Loss: 0.865268]\n",
      "1303 [D Loss: 0.590392, Acc.: 75.00%] [G Loss: 0.879407]\n",
      "1304 [D Loss: 0.605582, Acc.: 78.12%] [G Loss: 0.901751]\n",
      "1305 [D Loss: 0.619386, Acc.: 71.88%] [G Loss: 0.948261]\n",
      "1306 [D Loss: 0.564780, Acc.: 71.88%] [G Loss: 0.893941]\n",
      "1307 [D Loss: 0.630798, Acc.: 71.88%] [G Loss: 0.841935]\n",
      "1308 [D Loss: 0.564700, Acc.: 84.38%] [G Loss: 0.820691]\n",
      "1309 [D Loss: 0.564033, Acc.: 75.00%] [G Loss: 0.845080]\n",
      "1310 [D Loss: 0.571542, Acc.: 81.25%] [G Loss: 0.863154]\n",
      "1311 [D Loss: 0.582882, Acc.: 84.38%] [G Loss: 0.876994]\n",
      "1312 [D Loss: 0.595573, Acc.: 90.62%] [G Loss: 0.878035]\n",
      "1313 [D Loss: 0.595691, Acc.: 62.50%] [G Loss: 0.890902]\n",
      "1314 [D Loss: 0.576826, Acc.: 75.00%] [G Loss: 0.891283]\n",
      "1315 [D Loss: 0.586316, Acc.: 71.88%] [G Loss: 0.938881]\n",
      "1316 [D Loss: 0.499018, Acc.: 90.62%] [G Loss: 0.956757]\n",
      "1317 [D Loss: 0.591714, Acc.: 75.00%] [G Loss: 0.927722]\n",
      "1318 [D Loss: 0.622621, Acc.: 59.38%] [G Loss: 0.895895]\n",
      "1319 [D Loss: 0.626623, Acc.: 68.75%] [G Loss: 0.866842]\n",
      "1320 [D Loss: 0.608858, Acc.: 68.75%] [G Loss: 0.870944]\n",
      "1321 [D Loss: 0.609254, Acc.: 68.75%] [G Loss: 0.828021]\n",
      "1322 [D Loss: 0.589339, Acc.: 75.00%] [G Loss: 0.837190]\n",
      "1323 [D Loss: 0.597978, Acc.: 71.88%] [G Loss: 0.844077]\n",
      "1324 [D Loss: 0.579277, Acc.: 68.75%] [G Loss: 0.899655]\n",
      "1325 [D Loss: 0.634219, Acc.: 71.88%] [G Loss: 0.902779]\n",
      "1326 [D Loss: 0.665269, Acc.: 56.25%] [G Loss: 0.843425]\n",
      "1327 [D Loss: 0.598466, Acc.: 75.00%] [G Loss: 0.885005]\n",
      "1328 [D Loss: 0.582060, Acc.: 71.88%] [G Loss: 0.909531]\n",
      "1329 [D Loss: 0.593557, Acc.: 68.75%] [G Loss: 0.901240]\n",
      "1330 [D Loss: 0.581232, Acc.: 68.75%] [G Loss: 0.920677]\n",
      "1331 [D Loss: 0.618397, Acc.: 71.88%] [G Loss: 0.863818]\n",
      "1332 [D Loss: 0.624968, Acc.: 56.25%] [G Loss: 0.865820]\n",
      "1333 [D Loss: 0.636842, Acc.: 53.12%] [G Loss: 0.854668]\n",
      "1334 [D Loss: 0.575258, Acc.: 75.00%] [G Loss: 0.910464]\n",
      "1335 [D Loss: 0.561983, Acc.: 81.25%] [G Loss: 0.960822]\n",
      "1336 [D Loss: 0.647055, Acc.: 59.38%] [G Loss: 0.873949]\n",
      "1337 [D Loss: 0.577514, Acc.: 81.25%] [G Loss: 0.856165]\n",
      "1338 [D Loss: 0.572501, Acc.: 81.25%] [G Loss: 0.839353]\n",
      "1339 [D Loss: 0.576692, Acc.: 65.62%] [G Loss: 0.812774]\n",
      "1340 [D Loss: 0.578789, Acc.: 84.38%] [G Loss: 0.868178]\n",
      "1341 [D Loss: 0.541753, Acc.: 84.38%] [G Loss: 0.852660]\n",
      "1342 [D Loss: 0.666590, Acc.: 59.38%] [G Loss: 0.848418]\n",
      "1343 [D Loss: 0.693535, Acc.: 59.38%] [G Loss: 0.887211]\n",
      "1344 [D Loss: 0.587615, Acc.: 68.75%] [G Loss: 0.842397]\n",
      "1345 [D Loss: 0.575184, Acc.: 75.00%] [G Loss: 0.877434]\n",
      "1346 [D Loss: 0.619267, Acc.: 56.25%] [G Loss: 0.947715]\n",
      "1347 [D Loss: 0.616346, Acc.: 75.00%] [G Loss: 0.868750]\n",
      "1348 [D Loss: 0.680200, Acc.: 53.12%] [G Loss: 0.818049]\n",
      "1349 [D Loss: 0.646145, Acc.: 71.88%] [G Loss: 0.819113]\n",
      "1350 [D Loss: 0.560318, Acc.: 81.25%] [G Loss: 0.866857]\n",
      "1351 [D Loss: 0.629621, Acc.: 71.88%] [G Loss: 0.893450]\n",
      "1352 [D Loss: 0.598957, Acc.: 75.00%] [G Loss: 0.890641]\n",
      "1353 [D Loss: 0.627128, Acc.: 56.25%] [G Loss: 0.859261]\n",
      "1354 [D Loss: 0.565081, Acc.: 78.12%] [G Loss: 0.925184]\n",
      "1355 [D Loss: 0.576485, Acc.: 65.62%] [G Loss: 0.928016]\n",
      "1356 [D Loss: 0.553957, Acc.: 71.88%] [G Loss: 0.949823]\n",
      "1357 [D Loss: 0.636700, Acc.: 62.50%] [G Loss: 0.968428]\n",
      "1358 [D Loss: 0.608499, Acc.: 68.75%] [G Loss: 0.865018]\n",
      "1359 [D Loss: 0.553564, Acc.: 81.25%] [G Loss: 0.908083]\n",
      "1360 [D Loss: 0.630904, Acc.: 68.75%] [G Loss: 0.817531]\n",
      "1361 [D Loss: 0.574107, Acc.: 81.25%] [G Loss: 0.903032]\n",
      "1362 [D Loss: 0.560132, Acc.: 78.12%] [G Loss: 0.856379]\n",
      "1363 [D Loss: 0.574464, Acc.: 78.12%] [G Loss: 0.830669]\n",
      "1364 [D Loss: 0.608648, Acc.: 65.62%] [G Loss: 0.834948]\n",
      "1365 [D Loss: 0.602391, Acc.: 68.75%] [G Loss: 0.805635]\n",
      "1366 [D Loss: 0.574891, Acc.: 78.12%] [G Loss: 0.847762]\n",
      "1367 [D Loss: 0.631594, Acc.: 62.50%] [G Loss: 0.839375]\n",
      "1368 [D Loss: 0.625272, Acc.: 62.50%] [G Loss: 0.872142]\n",
      "1369 [D Loss: 0.629608, Acc.: 62.50%] [G Loss: 0.908375]\n",
      "1370 [D Loss: 0.580895, Acc.: 68.75%] [G Loss: 0.919148]\n",
      "1371 [D Loss: 0.572222, Acc.: 75.00%] [G Loss: 0.890431]\n",
      "1372 [D Loss: 0.617425, Acc.: 56.25%] [G Loss: 0.846425]\n",
      "1373 [D Loss: 0.640150, Acc.: 62.50%] [G Loss: 0.824892]\n",
      "1374 [D Loss: 0.630829, Acc.: 71.88%] [G Loss: 0.843257]\n",
      "1375 [D Loss: 0.579076, Acc.: 81.25%] [G Loss: 0.920569]\n",
      "1376 [D Loss: 0.606422, Acc.: 62.50%] [G Loss: 0.935202]\n",
      "1377 [D Loss: 0.581100, Acc.: 81.25%] [G Loss: 0.903397]\n",
      "1378 [D Loss: 0.564559, Acc.: 81.25%] [G Loss: 0.909656]\n",
      "1379 [D Loss: 0.609705, Acc.: 65.62%] [G Loss: 0.898594]\n",
      "1380 [D Loss: 0.646912, Acc.: 56.25%] [G Loss: 0.841060]\n",
      "1381 [D Loss: 0.548619, Acc.: 71.88%] [G Loss: 0.907210]\n",
      "1382 [D Loss: 0.577910, Acc.: 75.00%] [G Loss: 0.881475]\n",
      "1383 [D Loss: 0.651336, Acc.: 68.75%] [G Loss: 0.836760]\n",
      "1384 [D Loss: 0.594774, Acc.: 71.88%] [G Loss: 0.838080]\n",
      "1385 [D Loss: 0.612456, Acc.: 75.00%] [G Loss: 0.844428]\n",
      "1386 [D Loss: 0.642205, Acc.: 65.62%] [G Loss: 0.836514]\n",
      "1387 [D Loss: 0.624198, Acc.: 62.50%] [G Loss: 0.855353]\n",
      "1388 [D Loss: 0.606050, Acc.: 71.88%] [G Loss: 0.918071]\n",
      "1389 [D Loss: 0.616931, Acc.: 68.75%] [G Loss: 0.937499]\n",
      "1390 [D Loss: 0.575307, Acc.: 78.12%] [G Loss: 0.928719]\n",
      "1391 [D Loss: 0.636146, Acc.: 65.62%] [G Loss: 0.874775]\n",
      "1392 [D Loss: 0.539130, Acc.: 78.12%] [G Loss: 0.886917]\n",
      "1393 [D Loss: 0.644043, Acc.: 56.25%] [G Loss: 0.858946]\n",
      "1394 [D Loss: 0.581656, Acc.: 81.25%] [G Loss: 0.859788]\n",
      "1395 [D Loss: 0.631683, Acc.: 68.75%] [G Loss: 0.835902]\n",
      "1396 [D Loss: 0.576545, Acc.: 68.75%] [G Loss: 0.862638]\n",
      "1397 [D Loss: 0.591001, Acc.: 71.88%] [G Loss: 0.885562]\n",
      "1398 [D Loss: 0.608653, Acc.: 68.75%] [G Loss: 0.882111]\n",
      "1399 [D Loss: 0.597453, Acc.: 75.00%] [G Loss: 0.901253]\n",
      "1400 [D Loss: 0.604886, Acc.: 71.88%] [G Loss: 0.812366]\n",
      "1401 [D Loss: 0.522752, Acc.: 84.38%] [G Loss: 0.867538]\n",
      "1402 [D Loss: 0.686689, Acc.: 56.25%] [G Loss: 0.860628]\n",
      "1403 [D Loss: 0.558188, Acc.: 78.12%] [G Loss: 0.880443]\n",
      "1404 [D Loss: 0.647053, Acc.: 62.50%] [G Loss: 0.772609]\n",
      "1405 [D Loss: 0.559604, Acc.: 75.00%] [G Loss: 0.815144]\n",
      "1406 [D Loss: 0.566637, Acc.: 84.38%] [G Loss: 0.896683]\n",
      "1407 [D Loss: 0.589660, Acc.: 78.12%] [G Loss: 0.854646]\n",
      "1408 [D Loss: 0.613120, Acc.: 71.88%] [G Loss: 0.885674]\n",
      "1409 [D Loss: 0.543631, Acc.: 75.00%] [G Loss: 0.929606]\n",
      "1410 [D Loss: 0.626885, Acc.: 68.75%] [G Loss: 0.863226]\n",
      "1411 [D Loss: 0.537041, Acc.: 78.12%] [G Loss: 0.882891]\n",
      "1412 [D Loss: 0.606124, Acc.: 65.62%] [G Loss: 0.874191]\n",
      "1413 [D Loss: 0.637122, Acc.: 71.88%] [G Loss: 0.810670]\n",
      "1414 [D Loss: 0.601212, Acc.: 71.88%] [G Loss: 0.814879]\n",
      "1415 [D Loss: 0.577121, Acc.: 75.00%] [G Loss: 0.893120]\n",
      "1416 [D Loss: 0.578342, Acc.: 78.12%] [G Loss: 0.959268]\n",
      "1417 [D Loss: 0.533150, Acc.: 78.12%] [G Loss: 0.899785]\n",
      "1418 [D Loss: 0.648393, Acc.: 65.62%] [G Loss: 0.887992]\n",
      "1419 [D Loss: 0.582482, Acc.: 71.88%] [G Loss: 0.880986]\n",
      "1420 [D Loss: 0.646368, Acc.: 56.25%] [G Loss: 0.866497]\n",
      "1421 [D Loss: 0.607750, Acc.: 59.38%] [G Loss: 0.849030]\n",
      "1422 [D Loss: 0.651643, Acc.: 65.62%] [G Loss: 0.923181]\n",
      "1423 [D Loss: 0.624948, Acc.: 62.50%] [G Loss: 0.882665]\n",
      "1424 [D Loss: 0.575052, Acc.: 81.25%] [G Loss: 0.891117]\n",
      "1425 [D Loss: 0.546205, Acc.: 78.12%] [G Loss: 0.897084]\n",
      "1426 [D Loss: 0.620962, Acc.: 62.50%] [G Loss: 0.881842]\n",
      "1427 [D Loss: 0.572382, Acc.: 75.00%] [G Loss: 0.880349]\n",
      "1428 [D Loss: 0.602552, Acc.: 75.00%] [G Loss: 0.850443]\n",
      "1429 [D Loss: 0.644990, Acc.: 62.50%] [G Loss: 0.915947]\n",
      "1430 [D Loss: 0.637250, Acc.: 59.38%] [G Loss: 0.980717]\n",
      "1431 [D Loss: 0.634755, Acc.: 62.50%] [G Loss: 0.957501]\n",
      "1432 [D Loss: 0.550448, Acc.: 75.00%] [G Loss: 0.936200]\n",
      "1433 [D Loss: 0.597484, Acc.: 75.00%] [G Loss: 0.850099]\n",
      "1434 [D Loss: 0.598403, Acc.: 65.62%] [G Loss: 0.893062]\n",
      "1435 [D Loss: 0.658233, Acc.: 50.00%] [G Loss: 0.892603]\n",
      "1436 [D Loss: 0.615422, Acc.: 68.75%] [G Loss: 0.875168]\n",
      "1437 [D Loss: 0.587558, Acc.: 75.00%] [G Loss: 0.879989]\n",
      "1438 [D Loss: 0.616710, Acc.: 68.75%] [G Loss: 0.937764]\n",
      "1439 [D Loss: 0.568851, Acc.: 75.00%] [G Loss: 0.896621]\n",
      "1440 [D Loss: 0.607175, Acc.: 71.88%] [G Loss: 0.883562]\n",
      "1441 [D Loss: 0.600764, Acc.: 71.88%] [G Loss: 0.868767]\n",
      "1442 [D Loss: 0.576247, Acc.: 84.38%] [G Loss: 0.860831]\n",
      "1443 [D Loss: 0.675580, Acc.: 59.38%] [G Loss: 0.861044]\n",
      "1444 [D Loss: 0.591837, Acc.: 68.75%] [G Loss: 0.851517]\n",
      "1445 [D Loss: 0.642266, Acc.: 65.62%] [G Loss: 0.798488]\n",
      "1446 [D Loss: 0.644451, Acc.: 65.62%] [G Loss: 0.831959]\n",
      "1447 [D Loss: 0.559243, Acc.: 78.12%] [G Loss: 0.864018]\n",
      "1448 [D Loss: 0.525052, Acc.: 84.38%] [G Loss: 0.870971]\n",
      "1449 [D Loss: 0.655884, Acc.: 53.12%] [G Loss: 0.840469]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0e0b0afe65db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-1712d8b3c9fc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m      9\u001b[0m            \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m            \u001b[0mgen_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m            \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m            \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1226\u001b[0m     \u001b[1;34m\"\"\"Resets the state of metrics.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1228\u001b[1;33m       \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1230\u001b[0m   def train_on_batch(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3382\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3383\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3384\u001b[1;33m       \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3385\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3386\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m--> 848\u001b[1;33m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[0;32m    849\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    140\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m    141\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"AssignVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         tld.op_callbacks, resource, value)\n\u001b[0m\u001b[0;32m    143\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(epochs=100000,batch_size=32,save_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 28, 28, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = Input(shape=(28,28,1))\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,_),(_,_)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train,axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,x_train.shape[0],64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (64, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((64,1 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Generating image using the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=randn(100)\n",
    "vector=vector.reshape(1,100)\n",
    "X = model.predict(vector)\n",
    "pyplot.imshow(X[0,:,:,0],cmap='gray_r')\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
